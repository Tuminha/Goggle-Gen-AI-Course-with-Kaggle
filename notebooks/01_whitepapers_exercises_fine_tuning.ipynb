{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "bd3e7092",
      "metadata": {},
      "source": [
        "## Vertex AI Gemini Fine-Tuning (PEFT) Quickstart\n",
        "\n",
        "Based on the Feb 2025 Vertex AI documentation (\"Tune Gemini models with adapters\"), supervised\n",
        "fine-tuning currently supports Gemini 1.0 Pro and the preview Gemini 1.5 adapter variants in\n",
        "`us-central1`. This notebook follows that flow and replaces earlier cells that tried to fine-tune\n",
        "unsupported model identifiers.\n",
        "\n",
        "Before running the code cells:\n",
        "- Enable billing for your Google Cloud project.\n",
        "- Export `GOOGLE_CLOUD_PROJECT` (and optionally `GOOGLE_CLOUD_REGION`) or edit the constants in the\n",
        "  configuration cell.\n",
        "- Restart the runtime after any package upgrades.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "2553aab7",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Project: periospot-mvp\n",
            "Region: us-central1\n",
            "google-cloud-aiplatform: 1.119.0\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from datetime import datetime\n",
        "\n",
        "try:\n",
        "    from dotenv import load_dotenv\n",
        "except ModuleNotFoundError:\n",
        "    def load_dotenv():\n",
        "        return None\n",
        "\n",
        "import google.cloud.aiplatform as aiplatform\n",
        "import vertexai\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "PROJECT_ID = os.environ.get('GOOGLE_CLOUD_PROJECT') or os.environ.get('PROJECT_ID') or 'periospot-mvp'\n",
        "REGION = os.environ.get('GOOGLE_CLOUD_REGION', 'us-central1')\n",
        "\n",
        "if PROJECT_ID in {'', 'your-project-id', '<project_id>'}:\n",
        "    raise ValueError('Set PROJECT_ID or export GOOGLE_CLOUD_PROJECT before continuing.')\n",
        "\n",
        "if REGION != 'us-central1':\n",
        "    raise ValueError(\"Vertex AI Gemini supervised fine-tuning currently runs only in 'us-central1'.\")\n",
        "\n",
        "vertexai.init(project=PROJECT_ID, location=REGION)\n",
        "\n",
        "print(f'Project: {PROJECT_ID}')\n",
        "print(f'Region: {REGION}')\n",
        "print(f'google-cloud-aiplatform: {aiplatform.__version__}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "38479259",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîß Creating properly formatted training dataset...\n",
            "‚úÖ Created training file: /var/folders/cd/2c6x3jgj47j_fwl7231ts7m00000gn/T/tmpha9mjqre/training_data.jsonl\n",
            "\n",
            "üì§ Uploading training data to GCS...\n",
            "‚úÖ Using existing bucket: periospot-mvp-gemini-tuning\n",
            "‚úÖ Uploaded training data to: gs://periospot-mvp-gemini-tuning/training_data/training_20251007_152358.jsonl\n",
            "\n",
            "‚úÖ Configuration:\n",
            "Base model: gemini-2.5-pro (GA)\n",
            "Training dataset: gs://periospot-mvp-gemini-tuning/training_data/training_20251007_152358.jsonl\n",
            "Tuned model display name: gemini-peft-20251007-152359\n",
            "\n",
            "üéâ Ready for fine-tuning with properly formatted dataset!\n"
          ]
        }
      ],
      "source": [
        "from vertexai.generative_models import GenerativeModel\n",
        "from vertexai.preview.tuning import sft\n",
        "import json\n",
        "from google.cloud import storage\n",
        "\n",
        "SUPPORTED_BASE_MODELS = {\n",
        "    'gemini-2.5-pro': 'GA',\n",
        "    'gemini-2.5-flash': 'GA',\n",
        "    'gemini-2.5-flash-lite': 'GA',\n",
        "    'gemini-1.0-pro-002': 'GA',\n",
        "    'gemini-1.5-pro-002': 'Preview',\n",
        "    'gemini-1.5-flash-002': 'Preview',\n",
        "    'text-bison@002': 'Legacy',\n",
        "}\n",
        "\n",
        "# ‚ö†Ô∏è THE GOOGLE SAMPLE DATASET HAS WRONG FORMAT!\n",
        "# Old format: {\"input_text\": \"...\", \"output_text\": \"...\"}\n",
        "# New format for Gemini 2.5: {\"contents\": [{\"role\": \"user\", \"parts\": [{\"text\": \"...\"}]}, ...]}\n",
        "\n",
        "# Let's create a properly formatted training dataset\n",
        "print(\"üîß Creating properly formatted training dataset...\")\n",
        "\n",
        "# Sample training data in CORRECT format for Gemini 2.5\n",
        "training_examples = [\n",
        "    {\n",
        "        \"contents\": [\n",
        "            {\"role\": \"user\", \"parts\": [{\"text\": \"What is machine learning?\"}]},\n",
        "            {\"role\": \"model\", \"parts\": [{\"text\": \"Machine learning is a subset of artificial intelligence (AI) that enables computers to learn and improve from experience without being explicitly programmed. It uses algorithms and statistical models to analyze and draw inferences from patterns in data.\"}]}\n",
        "        ]\n",
        "    },\n",
        "    {\n",
        "        \"contents\": [\n",
        "            {\"role\": \"user\", \"parts\": [{\"text\": \"Explain neural networks in simple terms.\"}]},\n",
        "            {\"role\": \"model\", \"parts\": [{\"text\": \"Neural networks are computing systems inspired by the biological neural networks in animal brains. They consist of interconnected nodes (neurons) organized in layers that process information. These networks can learn to perform tasks by analyzing examples, gradually improving their accuracy through a process called training.\"}]}\n",
        "        ]\n",
        "    },\n",
        "    {\n",
        "        \"contents\": [\n",
        "            {\"role\": \"user\", \"parts\": [{\"text\": \"What is fine-tuning in AI?\"}]},\n",
        "            {\"role\": \"model\", \"parts\": [{\"text\": \"Fine-tuning is a process where you take a pre-trained AI model and train it further on a specific dataset relevant to your task. This allows the model to adapt its knowledge to perform better on particular applications while retaining the general capabilities it learned during initial training.\"}]}\n",
        "        ]\n",
        "    },\n",
        "]\n",
        "\n",
        "# Save to a local file\n",
        "import tempfile\n",
        "import os\n",
        "\n",
        "# Create temp directory for training data\n",
        "temp_dir = tempfile.mkdtemp()\n",
        "local_training_file = os.path.join(temp_dir, 'training_data.jsonl')\n",
        "\n",
        "with open(local_training_file, 'w') as f:\n",
        "    for example in training_examples:\n",
        "        f.write(json.dumps(example) + '\\n')\n",
        "\n",
        "print(f\"‚úÖ Created training file: {local_training_file}\")\n",
        "\n",
        "# ‚ö†Ô∏è MUST upload to GCS - local files don't work!\n",
        "print(\"\\nüì§ Uploading training data to GCS...\")\n",
        "\n",
        "# Create GCS bucket name (must be globally unique)\n",
        "BUCKET_NAME = f'{PROJECT_ID}-gemini-tuning'\n",
        "GCS_TRAINING_PATH = f'gs://{BUCKET_NAME}/training_data/training_{datetime.utcnow():%Y%m%d_%H%M%S}.jsonl'\n",
        "\n",
        "try:\n",
        "    # Initialize storage client\n",
        "    storage_client = storage.Client(project=PROJECT_ID)\n",
        "    \n",
        "    # Create bucket if it doesn't exist\n",
        "    try:\n",
        "        bucket = storage_client.create_bucket(BUCKET_NAME, location='us-central1')\n",
        "        print(f\"‚úÖ Created bucket: {BUCKET_NAME}\")\n",
        "    except Exception as e:\n",
        "        if 'already exists' in str(e).lower() or '409' in str(e):\n",
        "            bucket = storage_client.bucket(BUCKET_NAME)\n",
        "            print(f\"‚úÖ Using existing bucket: {BUCKET_NAME}\")\n",
        "        else:\n",
        "            raise\n",
        "    \n",
        "    # Upload the training file\n",
        "    blob_name = GCS_TRAINING_PATH.replace(f'gs://{BUCKET_NAME}/', '')\n",
        "    blob = bucket.blob(blob_name)\n",
        "    blob.upload_from_filename(local_training_file)\n",
        "    \n",
        "    print(f\"‚úÖ Uploaded training data to: {GCS_TRAINING_PATH}\")\n",
        "    \n",
        "    TRAINING_DATASET = GCS_TRAINING_PATH\n",
        "    VALIDATION_DATASET = None\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error uploading to GCS: {e}\")\n",
        "    print(\"\\nüí° ALTERNATIVE: Create your bucket manually:\")\n",
        "    print(f\"   1. Go to https://console.cloud.google.com/storage\")\n",
        "    print(f\"   2. Create bucket: {BUCKET_NAME}\")\n",
        "    print(f\"   3. Upload {local_training_file} to the bucket\")\n",
        "    print(f\"   4. Update TRAINING_DATASET = 'gs://{BUCKET_NAME}/your_file.jsonl'\")\n",
        "    raise\n",
        "\n",
        "BASE_MODEL = 'gemini-2.5-pro'\n",
        "TUNED_MODEL_DISPLAY_NAME = f'gemini-peft-{datetime.utcnow():%Y%m%d-%H%M%S}'\n",
        "\n",
        "if BASE_MODEL not in SUPPORTED_BASE_MODELS:\n",
        "    raise ValueError(f'Unsupported base model {BASE_MODEL}. Choose one of {list(SUPPORTED_BASE_MODELS)}.')\n",
        "\n",
        "print(f'\\n‚úÖ Configuration:')\n",
        "print(f'Base model: {BASE_MODEL} ({SUPPORTED_BASE_MODELS[BASE_MODEL]})')\n",
        "print(f'Training dataset: {TRAINING_DATASET}')\n",
        "print(f'Tuned model display name: {TUNED_MODEL_DISPLAY_NAME}')\n",
        "print(f'\\nüéâ Ready for fine-tuning with properly formatted dataset!')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "aeefd555",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üöÄ Starting fine-tuning job...\n",
            "Creating SupervisedTuningJob\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1759850639.604962 16652295 alts_credentials.cc:93] ALTS creds ignored. Not running on GCP and untrusted ALTS is not enabled.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SupervisedTuningJob created. Resource name: projects/1020593801304/locations/us-central1/tuningJobs/1542662865863835648\n",
            "To use this SupervisedTuningJob in another session:\n",
            "tuning_job = sft.SupervisedTuningJob('projects/1020593801304/locations/us-central1/tuningJobs/1542662865863835648')\n",
            "View Tuning Job:\n",
            "https://console.cloud.google.com/vertex-ai/generative/language/locations/us-central1/tuning/tuningJob/1542662865863835648?project=1020593801304\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "        \n",
              "    <link rel=\"stylesheet\" href=\"https://fonts.googleapis.com/icon?family=Material+Icons\">\n",
              "    <style>\n",
              "      .view-vertex-resource,\n",
              "      .view-vertex-resource:hover,\n",
              "      .view-vertex-resource:visited {\n",
              "        position: relative;\n",
              "        display: inline-flex;\n",
              "        flex-direction: row;\n",
              "        height: 32px;\n",
              "        padding: 0 12px;\n",
              "          margin: 4px 18px;\n",
              "        gap: 4px;\n",
              "        border-radius: 4px;\n",
              "\n",
              "        align-items: center;\n",
              "        justify-content: center;\n",
              "        background-color: rgb(255, 255, 255);\n",
              "        color: rgb(51, 103, 214);\n",
              "\n",
              "        font-family: Roboto,\"Helvetica Neue\",sans-serif;\n",
              "        font-size: 13px;\n",
              "        font-weight: 500;\n",
              "        text-transform: uppercase;\n",
              "        text-decoration: none !important;\n",
              "\n",
              "        transition: box-shadow 280ms cubic-bezier(0.4, 0, 0.2, 1) 0s;\n",
              "        box-shadow: 0px 3px 1px -2px rgba(0,0,0,0.2), 0px 2px 2px 0px rgba(0,0,0,0.14), 0px 1px 5px 0px rgba(0,0,0,0.12);\n",
              "      }\n",
              "      .view-vertex-resource:active {\n",
              "        box-shadow: 0px 5px 5px -3px rgba(0,0,0,0.2),0px 8px 10px 1px rgba(0,0,0,0.14),0px 3px 14px 2px rgba(0,0,0,0.12);\n",
              "      }\n",
              "      .view-vertex-resource:active .view-vertex-ripple::before {\n",
              "        position: absolute;\n",
              "        top: 0;\n",
              "        bottom: 0;\n",
              "        left: 0;\n",
              "        right: 0;\n",
              "        border-radius: 4px;\n",
              "        pointer-events: none;\n",
              "\n",
              "        content: '';\n",
              "        background-color: rgb(51, 103, 214);\n",
              "        opacity: 0.12;\n",
              "      }\n",
              "      .view-vertex-icon {\n",
              "        font-size: 18px;\n",
              "      }\n",
              "    </style>\n",
              "  \n",
              "        <a class=\"view-vertex-resource\" id=\"view-vertex-resource-6a58c5a8-eff5-4a3a-a2ed-9a274bb5fbe8\" href=\"#view-view-vertex-resource-6a58c5a8-eff5-4a3a-a2ed-9a274bb5fbe8\">\n",
              "          <span class=\"material-icons view-vertex-icon\">tune</span>\n",
              "          <span>View Tuning Job</span>\n",
              "        </a>\n",
              "        \n",
              "        <script>\n",
              "          (function () {\n",
              "            const link = document.getElementById('view-vertex-resource-6a58c5a8-eff5-4a3a-a2ed-9a274bb5fbe8');\n",
              "            link.addEventListener('click', (e) => {\n",
              "              if (window.google?.colab?.openUrl) {\n",
              "                window.google.colab.openUrl('https://console.cloud.google.com/vertex-ai/generative/language/locations/us-central1/tuning/tuningJob/1542662865863835648?project=1020593801304');\n",
              "              } else {\n",
              "                window.open('https://console.cloud.google.com/vertex-ai/generative/language/locations/us-central1/tuning/tuningJob/1542662865863835648?project=1020593801304', '_blank');\n",
              "              }\n",
              "              e.stopPropagation();\n",
              "              e.preventDefault();\n",
              "            });\n",
              "          })();\n",
              "        </script>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Started tuning job: projects/1020593801304/locations/us-central1/tuningJobs/1542662865863835648\n",
            "üìä Job name: 1542662865863835648\n",
            "‚è∞ This may take 15-30 minutes to complete.\n",
            "\n",
            "üîó Monitor progress at:\n",
            "   https://console.cloud.google.com/vertex-ai/locations/us-central1/tuning-jobs/1542662865863835648?project=periospot-mvp\n",
            "\n",
            "‚è≥ Monitoring job status (this will wait until completion)...\n",
            "‚è±Ô∏è Status: JOB_STATE_PENDING\n",
            "‚è±Ô∏è Status: JOB_STATE_PENDING\n",
            "‚è±Ô∏è Status: JOB_STATE_PENDING\n",
            "‚è±Ô∏è Status: JOB_STATE_PENDING\n",
            "‚è±Ô∏è Status: JOB_STATE_PENDING\n",
            "‚è±Ô∏è Status: JOB_STATE_PENDING\n",
            "‚è±Ô∏è Status: JOB_STATE_PENDING\n",
            "‚è±Ô∏è Status: JOB_STATE_PENDING\n",
            "‚è±Ô∏è Status: JOB_STATE_PENDING\n",
            "‚è±Ô∏è Status: JOB_STATE_PENDING\n",
            "‚è±Ô∏è Status: JOB_STATE_PENDING\n",
            "‚è±Ô∏è Status: JOB_STATE_PENDING\n",
            "‚è±Ô∏è Status: JOB_STATE_PENDING\n",
            "‚è±Ô∏è Status: JOB_STATE_PENDING\n",
            "‚è±Ô∏è Status: JOB_STATE_PENDING\n",
            "‚è±Ô∏è Status: JOB_STATE_PENDING\n",
            "‚è±Ô∏è Status: JOB_STATE_PENDING\n",
            "‚è±Ô∏è Status: JOB_STATE_PENDING\n",
            "‚è±Ô∏è Status: JOB_STATE_PENDING\n",
            "‚è±Ô∏è Status: JOB_STATE_PENDING\n",
            "‚è±Ô∏è Status: JOB_STATE_PENDING\n",
            "‚è±Ô∏è Status: JOB_STATE_PENDING\n",
            "‚è±Ô∏è Status: JOB_STATE_PENDING\n",
            "‚è±Ô∏è Status: JOB_STATE_PENDING\n",
            "‚è±Ô∏è Status: JOB_STATE_PENDING\n",
            "‚è±Ô∏è Status: JOB_STATE_PENDING\n",
            "‚è±Ô∏è Status: JOB_STATE_PENDING\n",
            "‚è±Ô∏è Status: JOB_STATE_PENDING\n",
            "‚è±Ô∏è Status: JOB_STATE_PENDING\n",
            "‚è±Ô∏è Status: JOB_STATE_PENDING\n",
            "‚è±Ô∏è Status: JOB_STATE_PENDING\n",
            "‚è±Ô∏è Status: JOB_STATE_PENDING\n",
            "‚è±Ô∏è Status: JOB_STATE_PENDING\n",
            "‚è±Ô∏è Status: JOB_STATE_PENDING\n",
            "‚è±Ô∏è Status: JOB_STATE_PENDING\n",
            "‚è±Ô∏è Status: JOB_STATE_PENDING\n",
            "‚è±Ô∏è Status: JOB_STATE_PENDING\n",
            "‚è±Ô∏è Status: JOB_STATE_PENDING\n",
            "‚è±Ô∏è Status: JOB_STATE_PENDING\n",
            "‚è±Ô∏è Status: JOB_STATE_PENDING\n",
            "‚è±Ô∏è Status: JOB_STATE_PENDING\n",
            "‚è±Ô∏è Status: JOB_STATE_PENDING\n",
            "‚è±Ô∏è Status: JOB_STATE_PENDING\n",
            "‚è±Ô∏è Status: JOB_STATE_PENDING\n",
            "‚è±Ô∏è Status: JOB_STATE_PENDING\n",
            "‚è±Ô∏è Status: JOB_STATE_PENDING\n",
            "‚è±Ô∏è Status: JOB_STATE_PENDING\n",
            "‚è±Ô∏è Status: JOB_STATE_PENDING\n",
            "‚è±Ô∏è Status: JOB_STATE_PENDING\n",
            "‚è±Ô∏è Status: JOB_STATE_PENDING\n",
            "‚è±Ô∏è Status: JOB_STATE_PENDING\n",
            "‚è±Ô∏è Status: JOB_STATE_PENDING\n",
            "‚è±Ô∏è Status: JOB_STATE_PENDING\n",
            "‚è±Ô∏è Status: JOB_STATE_PENDING\n",
            "‚è±Ô∏è Status: JOB_STATE_PENDING\n",
            "‚è±Ô∏è Status: JOB_STATE_PENDING\n",
            "‚è±Ô∏è Status: JOB_STATE_PENDING\n",
            "‚è±Ô∏è Status: JOB_STATE_PENDING\n",
            "‚è±Ô∏è Status: JOB_STATE_PENDING\n",
            "‚è±Ô∏è Status: JOB_STATE_PENDING\n",
            "‚è±Ô∏è Status: JOB_STATE_PENDING\n",
            "‚è±Ô∏è Status: JOB_STATE_PENDING\n",
            "‚è±Ô∏è Status: JOB_STATE_PENDING\n",
            "‚è±Ô∏è Status: JOB_STATE_PENDING\n",
            "‚è±Ô∏è Status: JOB_STATE_PENDING\n",
            "‚è±Ô∏è Status: JOB_STATE_PENDING\n",
            "‚è±Ô∏è Status: JOB_STATE_PENDING\n",
            "‚è±Ô∏è Status: JOB_STATE_PENDING\n",
            "‚è±Ô∏è Status: JOB_STATE_PENDING\n",
            "‚è±Ô∏è Status: JOB_STATE_PENDING\n",
            "‚è±Ô∏è Status: JOB_STATE_PENDING\n",
            "‚è±Ô∏è Status: JOB_STATE_PENDING\n",
            "‚è±Ô∏è Status: JOB_STATE_PENDING\n",
            "‚è±Ô∏è Status: JOB_STATE_PENDING\n",
            "‚è±Ô∏è Status: JOB_STATE_PENDING\n",
            "‚è±Ô∏è Status: JOB_STATE_PENDING\n",
            "‚è±Ô∏è Status: JOB_STATE_PENDING\n",
            "‚è±Ô∏è Status: JOB_STATE_PENDING\n",
            "‚è±Ô∏è Status: JOB_STATE_PENDING\n",
            "‚è±Ô∏è Status: JOB_STATE_PENDING\n",
            "‚è±Ô∏è Status: JOB_STATE_PENDING\n",
            "‚è±Ô∏è Status: JOB_STATE_PENDING\n",
            "‚è±Ô∏è Status: JOB_STATE_PENDING\n",
            "‚è±Ô∏è Status: JOB_STATE_PENDING\n",
            "‚è±Ô∏è Status: JOB_STATE_PENDING\n",
            "‚è±Ô∏è Status: JOB_STATE_PENDING\n",
            "‚è±Ô∏è Status: JOB_STATE_PENDING\n",
            "‚è±Ô∏è Status: JOB_STATE_PENDING\n",
            "‚è±Ô∏è Status: JOB_STATE_PENDING\n",
            "‚è±Ô∏è Status: JOB_STATE_PENDING\n",
            "‚è±Ô∏è Status: JOB_STATE_PENDING\n",
            "‚è±Ô∏è Status: JOB_STATE_PENDING\n",
            "‚è±Ô∏è Status: JOB_STATE_PENDING\n",
            "‚è±Ô∏è Status: JOB_STATE_PENDING\n",
            "‚è±Ô∏è Status: JOB_STATE_PENDING\n",
            "‚è±Ô∏è Status: JOB_STATE_PENDING\n",
            "‚è±Ô∏è Status: JOB_STATE_PENDING\n",
            "‚è±Ô∏è Status: JOB_STATE_PENDING\n",
            "‚è±Ô∏è Status: JOB_STATE_PENDING\n",
            "‚è±Ô∏è Status: JOB_STATE_PENDING\n",
            "‚è±Ô∏è Status: JOB_STATE_PENDING\n",
            "‚è±Ô∏è Status: JOB_STATE_PENDING\n",
            "‚è±Ô∏è Status: JOB_STATE_PENDING\n",
            "‚è±Ô∏è Status: JOB_STATE_PENDING\n",
            "‚è±Ô∏è Status: JOB_STATE_PENDING\n",
            "‚è±Ô∏è Status: JOB_STATE_PENDING\n",
            "‚è±Ô∏è Status: JOB_STATE_PENDING\n",
            "‚è±Ô∏è Status: JOB_STATE_PENDING\n",
            "‚è±Ô∏è Status: JOB_STATE_PENDING\n",
            "‚è±Ô∏è Status: JOB_STATE_PENDING\n",
            "‚è±Ô∏è Status: JOB_STATE_PENDING\n",
            "‚è±Ô∏è Status: JOB_STATE_PENDING\n",
            "‚è±Ô∏è Status: JOB_STATE_PENDING\n",
            "‚è±Ô∏è Status: JOB_STATE_PENDING\n",
            "‚è±Ô∏è Status: JOB_STATE_PENDING\n",
            "‚è±Ô∏è Status: JOB_STATE_PENDING\n",
            "‚è±Ô∏è Status: JOB_STATE_PENDING\n",
            "‚è±Ô∏è Status: JOB_STATE_PENDING\n",
            "‚è±Ô∏è Status: JOB_STATE_PENDING\n",
            "‚è±Ô∏è Status: JOB_STATE_PENDING\n",
            "‚è±Ô∏è Status: JOB_STATE_PENDING\n",
            "‚è∞ Timeout after 60.0 minutes\n",
            "üí° Job is still running. Check Cloud Console for status.\n",
            "\n",
            "‚ö†Ô∏è Model endpoint not yet available\n",
            "üí° Check the Cloud Console URL above for the endpoint once training completes\n"
          ]
        }
      ],
      "source": [
        "train_kwargs = {\n",
        "    'source_model': BASE_MODEL,\n",
        "    'train_dataset': TRAINING_DATASET,\n",
        "    'tuned_model_display_name': TUNED_MODEL_DISPLAY_NAME\n",
        "}\n",
        "if VALIDATION_DATASET:\n",
        "    train_kwargs['validation_dataset'] = VALIDATION_DATASET\n",
        "\n",
        "print(\"üöÄ Starting fine-tuning job...\")\n",
        "tuning_job = sft.train(**train_kwargs)\n",
        "\n",
        "print(f'‚úÖ Started tuning job: {tuning_job.resource_name}')\n",
        "print(f'üìä Job name: {tuning_job.name}')\n",
        "print(f'‚è∞ This may take 15-30 minutes to complete.')\n",
        "\n",
        "# The job runs asynchronously - you can check status in the Cloud Console\n",
        "# URL: https://console.cloud.google.com/vertex-ai/generative/language/tuning-jobs\n",
        "print(f'\\nüîó Monitor progress at:')\n",
        "print(f'   https://console.cloud.google.com/vertex-ai/locations/{REGION}/tuning-jobs/{tuning_job.name.split(\"/\")[-1]}?project={PROJECT_ID}')\n",
        "\n",
        "# For monitoring in the notebook, you can poll the status\n",
        "print(f'\\n‚è≥ Monitoring job status (this will wait until completion)...')\n",
        "import time\n",
        "\n",
        "# Poll every 30 seconds\n",
        "max_wait_time = 3600  # 1 hour max\n",
        "start_time = time.time()\n",
        "check_interval = 30  # seconds\n",
        "\n",
        "while True:\n",
        "    # Refresh job status\n",
        "    try:\n",
        "        # Get current state\n",
        "        current_state = tuning_job.state\n",
        "        print(f\"‚è±Ô∏è Status: {current_state.name if hasattr(current_state, 'name') else current_state}\")\n",
        "        \n",
        "        # Check if job is done\n",
        "        if hasattr(current_state, 'name'):\n",
        "            state_name = current_state.name\n",
        "        else:\n",
        "            state_name = str(current_state)\n",
        "        \n",
        "        if 'SUCCEEDED' in state_name or 'JOB_STATE_SUCCEEDED' in state_name:\n",
        "            print(\"üéâ Fine-tuning completed successfully!\")\n",
        "            break\n",
        "        elif 'FAILED' in state_name or 'CANCELLED' in state_name:\n",
        "            print(f\"‚ùå Fine-tuning failed with state: {state_name}\")\n",
        "            break\n",
        "        \n",
        "        # Check timeout\n",
        "        elapsed = time.time() - start_time\n",
        "        if elapsed > max_wait_time:\n",
        "            print(f\"‚è∞ Timeout after {max_wait_time/60:.1f} minutes\")\n",
        "            print(\"üí° Job is still running. Check Cloud Console for status.\")\n",
        "            break\n",
        "        \n",
        "        # Wait before next check\n",
        "        time.sleep(check_interval)\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Error checking status: {e}\")\n",
        "        print(\"üí° Check Cloud Console for job status\")\n",
        "        break\n",
        "\n",
        "# Get the tuned model information\n",
        "try:\n",
        "    if hasattr(tuning_job, 'tuned_model_name') and tuning_job.tuned_model_name:\n",
        "        TUNED_MODEL_NAME = tuning_job.tuned_model_name\n",
        "        print(f\"\\n‚úÖ Tuned model name: {TUNED_MODEL_NAME}\")\n",
        "    \n",
        "    if hasattr(tuning_job, 'tuned_model_endpoint_name') and tuning_job.tuned_model_endpoint_name:\n",
        "        TUNED_MODEL_ENDPOINT = tuning_job.tuned_model_endpoint_name\n",
        "        print(f\"‚úÖ Tuned model endpoint: {TUNED_MODEL_ENDPOINT}\")\n",
        "    else:\n",
        "        print(f\"\\n‚ö†Ô∏è Model endpoint not yet available\")\n",
        "        print(f\"üí° Check the Cloud Console URL above for the endpoint once training completes\")\n",
        "        \n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è Could not retrieve tuned model info: {e}\")\n",
        "    print(f\"üí° You can get the endpoint from Cloud Console once training completes\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0342e8f8",
      "metadata": {},
      "source": [
        "## üéâ **SOLUTION**: Correctly Formatted Training Dataset\n",
        "\n",
        "### ‚ö†Ô∏è **Root Cause of Error**\n",
        "The error `\"Row: 0. Missing required 'contents' field\"` was caused by using Google's sample dataset which has an **OUTDATED FORMAT**.\n",
        "\n",
        "### ‚úÖ **Correct Format for Gemini 2.5**\n",
        "Each training example must use this structure:\n",
        "```json\n",
        "{\n",
        "  \"contents\": [\n",
        "    {\"role\": \"user\", \"parts\": [{\"text\": \"Your question here\"}]},\n",
        "    {\"role\": \"model\", \"parts\": [{\"text\": \"Expected response here\"}]}\n",
        "  ]\n",
        "}\n",
        "```\n",
        "\n",
        "The cell above creates a properly formatted dataset that will work with Gemini 2.5 fine-tuning!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "1b9f00c1",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üöÄ COMPREHENSIVE MODEL TESTING\n",
            "==================================================\n",
            "\n",
            "üîç Testing model: gemini-2.5-pro\n",
            "----------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/Users/franciscoteixeirabarbosa/.local/lib/python3.11/site-packages/vertexai/generative_models/_generative_models.py:433: UserWarning: This feature is deprecated as of June 24, 2025 and will be removed on June 24, 2026. For details, see https://cloud.google.com/vertex-ai/generative-ai/docs/deprecations/genai-vertexai-sdk.\n",
            "  warning_logs.show_deprecation_warning()\n",
            "E0000 00:00:1759854244.722820 16652295 alts_credentials.cc:93] ALTS creds ignored. Not running on GCP and untrusted ALTS is not enabled.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Inference: SUCCESS (31.73s)\n",
            "   Preview: Of course! Here is a detailed explanation of what an LLM is, broken down from a ...\n",
            "üîß Testing fine-tuning...\n",
            "Creating SupervisedTuningJob\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "E0000 00:00:1759854276.389246 16652295 alts_credentials.cc:93] ALTS creds ignored. Not running on GCP and untrusted ALTS is not enabled.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚ùå Fine-tuning: NOT SUPPORTED - 400 Row: 0. Missing required `contents` field....\n",
            "\n",
            "üîç Testing model: gemini-2.5-flash\n",
            "----------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "E0000 00:00:1759854278.193850 16652295 alts_credentials.cc:93] ALTS creds ignored. Not running on GCP and untrusted ALTS is not enabled.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Inference: SUCCESS (10.04s)\n",
            "   Preview: An **LLM** stands for **Large Language Model**.\n",
            "\n",
            "It's a type of artificial intel...\n",
            "üîß Testing fine-tuning...\n",
            "Creating SupervisedTuningJob\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "E0000 00:00:1759854288.236561 16652295 alts_credentials.cc:93] ALTS creds ignored. Not running on GCP and untrusted ALTS is not enabled.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚ùå Fine-tuning: NOT SUPPORTED - 400 Row: 0. Missing required `contents` field....\n",
            "\n",
            "üîç Testing model: gemini-2.5-flash-lite\n",
            "----------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "E0000 00:00:1759854289.837409 16652295 alts_credentials.cc:93] ALTS creds ignored. Not running on GCP and untrusted ALTS is not enabled.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Inference: SUCCESS (13.52s)\n",
            "   Preview: LLM stands for **Large Language Model**.\n",
            "\n",
            "In simpler terms, an LLM is a type of ...\n",
            "üîß Testing fine-tuning...\n",
            "Creating SupervisedTuningJob\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "E0000 00:00:1759854303.366424 16652295 alts_credentials.cc:93] ALTS creds ignored. Not running on GCP and untrusted ALTS is not enabled.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚ùå Fine-tuning: NOT SUPPORTED - 400 Row: 0. Missing required `contents` field....\n",
            "\n",
            "üîç Testing model: gemini-2.0-flash\n",
            "----------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "E0000 00:00:1759854304.300747 16652295 alts_credentials.cc:93] ALTS creds ignored. Not running on GCP and untrusted ALTS is not enabled.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Inference: SUCCESS (5.67s)\n",
            "   Preview: LLM stands for **Large Language Model**. It's a type of artificial intelligence ...\n",
            "üîß Testing fine-tuning...\n",
            "Creating SupervisedTuningJob\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "E0000 00:00:1759854309.974637 16652295 alts_credentials.cc:93] ALTS creds ignored. Not running on GCP and untrusted ALTS is not enabled.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚ùå Fine-tuning: NOT SUPPORTED - 400 Base model gemini-2.0-flash is not supported....\n",
            "\n",
            "üîç Testing model: gemini-2.0-flash-exp\n",
            "----------------------------------------\n",
            "‚ùå Inference: FAILED - 404 Publisher Model `projects/periospot-mvp/locations/europe-west1/publishers/google/models/gemini-2...\n",
            "\n",
            "üîç Testing model: gemini-2.0-flash-thinking-exp\n",
            "----------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "E0000 00:00:1759854310.313937 16652295 alts_credentials.cc:93] ALTS creds ignored. Not running on GCP and untrusted ALTS is not enabled.\n",
            "E0000 00:00:1759854310.468157 16652295 alts_credentials.cc:93] ALTS creds ignored. Not running on GCP and untrusted ALTS is not enabled.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚ùå Inference: FAILED - 404 Publisher Model `projects/periospot-mvp/locations/europe-west1/publishers/google/models/gemini-2...\n",
            "\n",
            "üîç Testing model: gemini-1.5-flash\n",
            "----------------------------------------\n",
            "‚ùå Inference: FAILED - 404 Publisher Model `projects/periospot-mvp/locations/europe-west1/publishers/google/models/gemini-1...\n",
            "\n",
            "üîç Testing model: gemini-1.5-flash-8b\n",
            "----------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "E0000 00:00:1759854310.588994 16652295 alts_credentials.cc:93] ALTS creds ignored. Not running on GCP and untrusted ALTS is not enabled.\n",
            "E0000 00:00:1759854310.788018 16652295 alts_credentials.cc:93] ALTS creds ignored. Not running on GCP and untrusted ALTS is not enabled.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚ùå Inference: FAILED - 404 Publisher Model `projects/periospot-mvp/locations/europe-west1/publishers/google/models/gemini-1...\n",
            "\n",
            "üîç Testing model: gemini-1.5-pro\n",
            "----------------------------------------\n",
            "‚ùå Inference: FAILED - 404 Publisher Model `projects/periospot-mvp/locations/europe-west1/publishers/google/models/gemini-1...\n",
            "\n",
            "üîç Testing model: gemini-1.5-pro-latest\n",
            "----------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "E0000 00:00:1759854310.943448 16652295 alts_credentials.cc:93] ALTS creds ignored. Not running on GCP and untrusted ALTS is not enabled.\n",
            "E0000 00:00:1759854311.100593 16652295 alts_credentials.cc:93] ALTS creds ignored. Not running on GCP and untrusted ALTS is not enabled.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚ùå Inference: FAILED - 404 Publisher Model `projects/periospot-mvp/locations/europe-west1/publishers/google/models/gemini-1...\n",
            "\n",
            "üîç Testing model: gemini-1.0-pro\n",
            "----------------------------------------\n",
            "‚ùå Inference: FAILED - 404 Publisher Model `projects/periospot-mvp/locations/europe-west1/publishers/google/models/gemini-1...\n",
            "\n",
            "üîç Testing model: gemini-1.0-pro-latest\n",
            "----------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "E0000 00:00:1759854311.227654 16652295 alts_credentials.cc:93] ALTS creds ignored. Not running on GCP and untrusted ALTS is not enabled.\n",
            "E0000 00:00:1759854311.381949 16652295 alts_credentials.cc:93] ALTS creds ignored. Not running on GCP and untrusted ALTS is not enabled.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚ùå Inference: FAILED - 404 Publisher Model `projects/periospot-mvp/locations/europe-west1/publishers/google/models/gemini-1...\n",
            "\n",
            "üîç Testing model: gemini-exp-1206\n",
            "----------------------------------------\n",
            "‚ùå Inference: FAILED - 404 Publisher Model `projects/periospot-mvp/locations/europe-west1/publishers/google/models/gemini-e...\n",
            "\n",
            "üîç Testing model: gemini-exp-1206-pro\n",
            "----------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "E0000 00:00:1759854311.542916 16652295 alts_credentials.cc:93] ALTS creds ignored. Not running on GCP and untrusted ALTS is not enabled.\n",
            "E0000 00:00:1759854311.709391 16652295 alts_credentials.cc:93] ALTS creds ignored. Not running on GCP and untrusted ALTS is not enabled.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚ùå Inference: FAILED - 404 Publisher Model `projects/periospot-mvp/locations/europe-west1/publishers/google/models/gemini-e...\n",
            "\n",
            "============================================================\n",
            "üìä TESTING SUMMARY REPORT\n",
            "============================================================\n",
            "\n",
            "üîπ INFERENCE RESULTS:\n",
            "------------------------------\n",
            "‚úÖ gemini-2.5-pro: ‚úÖ SUCCESS\n",
            "‚úÖ gemini-2.5-flash: ‚úÖ SUCCESS\n",
            "‚úÖ gemini-2.5-flash-lite: ‚úÖ SUCCESS\n",
            "‚úÖ gemini-2.0-flash: ‚úÖ SUCCESS\n",
            "‚ùå gemini-2.0-flash-exp: ‚ùå FAILED\n",
            "‚ùå gemini-2.0-flash-thinking-exp: ‚ùå FAILED\n",
            "‚ùå gemini-1.5-flash: ‚ùå FAILED\n",
            "‚ùå gemini-1.5-flash-8b: ‚ùå FAILED\n",
            "‚ùå gemini-1.5-pro: ‚ùå FAILED\n",
            "‚ùå gemini-1.5-pro-latest: ‚ùå FAILED\n",
            "‚ùå gemini-1.0-pro: ‚ùå FAILED\n",
            "‚ùå gemini-1.0-pro-latest: ‚ùå FAILED\n",
            "‚ùå gemini-exp-1206: ‚ùå FAILED\n",
            "‚ùå gemini-exp-1206-pro: ‚ùå FAILED\n",
            "\n",
            "üîπ FINE-TUNING RESULTS:\n",
            "------------------------------\n",
            "‚ùå gemini-2.5-pro: ‚ùå NOT SUPPORTED\n",
            "‚ùå gemini-2.5-flash: ‚ùå NOT SUPPORTED\n",
            "‚ùå gemini-2.5-flash-lite: ‚ùå NOT SUPPORTED\n",
            "‚ùå gemini-2.0-flash: ‚ùå NOT SUPPORTED\n",
            "\n",
            "üéØ RECOMMENDATIONS:\n",
            "--------------------\n",
            "‚úÖ Use these models for inference: gemini-2.5-pro, gemini-2.5-flash, gemini-2.5-flash-lite\n",
            "‚ùå No models support fine-tuning in this region/project\n",
            "üí° Try using AI Studio API instead of Vertex AI for fine-tuning\n",
            "\n",
            "üìà Total tested: 14 models\n",
            "‚úÖ Inference working: 4 models\n",
            "üîß Fine-tuning supported: 0 models\n"
          ]
        }
      ],
      "source": [
        "# üîç COMPREHENSIVE MODEL TESTING SYSTEM\n",
        "# This cell tests which models work for inference and fine-tuning\n",
        "\n",
        "import vertexai\n",
        "from vertexai.generative_models import GenerativeModel\n",
        "from vertexai.preview.tuning import sft\n",
        "import time\n",
        "\n",
        "# Initialize Vertex AI\n",
        "PROJECT_ID = 'periospot-mvp'\n",
        "REGION = 'europe-west1'\n",
        "vertexai.init(project=PROJECT_ID, location=REGION)\n",
        "\n",
        "# List of models to test - UPDATED with correct Gemini 2.5 models\n",
        "MODELS_TO_TEST = [\n",
        "    # Gemini 2.5 models (SUPPORTED for fine-tuning)\n",
        "    'gemini-2.5-pro',\n",
        "    'gemini-2.5-flash',\n",
        "    'gemini-2.5-flash-lite',\n",
        "    \n",
        "    # Gemini 2.0 models (inference only)\n",
        "    'gemini-2.0-flash',\n",
        "    'gemini-2.0-flash-exp',\n",
        "    'gemini-2.0-flash-thinking-exp',\n",
        "    \n",
        "    # Gemini 1.5 models (legacy, may not be available)\n",
        "    'gemini-1.5-flash',\n",
        "    'gemini-1.5-flash-8b',\n",
        "    'gemini-1.5-pro',\n",
        "    'gemini-1.5-pro-latest',\n",
        "    \n",
        "    # Gemini 1.0 models (legacy)\n",
        "    'gemini-1.0-pro',\n",
        "    'gemini-1.0-pro-latest',\n",
        "    \n",
        "    # Experimental models\n",
        "    'gemini-exp-1206',\n",
        "    'gemini-exp-1206-pro',\n",
        "]\n",
        "\n",
        "# Training dataset for fine-tuning tests\n",
        "TRAINING_DATASET = 'gs://cloud-samples-data/vertex-ai/model-evaluation/peft_train_sample.jsonl'\n",
        "\n",
        "print(\"üöÄ COMPREHENSIVE MODEL TESTING\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Results storage\n",
        "inference_results = {}\n",
        "fine_tuning_results = {}\n",
        "\n",
        "for model_name in MODELS_TO_TEST:\n",
        "    print(f\"\\nüîç Testing model: {model_name}\")\n",
        "    print(\"-\" * 40)\n",
        "    \n",
        "    # Test 1: Inference (generate_content)\n",
        "    try:\n",
        "        model = GenerativeModel(model_name)\n",
        "        start_time = time.time()\n",
        "        response = model.generate_content(\"What is a LLM?\")\n",
        "        end_time = time.time()\n",
        "        \n",
        "        inference_results[model_name] = {\n",
        "            'status': '‚úÖ SUCCESS',\n",
        "            'response_time': f\"{end_time - start_time:.2f}s\",\n",
        "            'response_preview': response.text[:100] + \"...\",\n",
        "            'error': None\n",
        "        }\n",
        "        print(f\"‚úÖ Inference: SUCCESS ({end_time - start_time:.2f}s)\")\n",
        "        print(f\"   Preview: {response.text[:80]}...\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        inference_results[model_name] = {\n",
        "            'status': '‚ùå FAILED',\n",
        "            'response_time': None,\n",
        "            'response_preview': None,\n",
        "            'error': str(e)\n",
        "        }\n",
        "        print(f\"‚ùå Inference: FAILED - {str(e)[:100]}...\")\n",
        "        continue  # Skip fine-tuning test if inference fails\n",
        "    \n",
        "    # Test 2: Fine-tuning (only if inference works)\n",
        "    try:\n",
        "        print(f\"üîß Testing fine-tuning...\")\n",
        "        tuning_job = sft.train(\n",
        "            source_model=model_name,\n",
        "            train_dataset=TRAINING_DATASET,\n",
        "            tuned_model_display_name=f'test-tuning-{model_name.replace(\"-\", \"_\")}',\n",
        "        )\n",
        "        \n",
        "        fine_tuning_results[model_name] = {\n",
        "            'status': '‚úÖ SUPPORTED',\n",
        "            'endpoint': tuning_job.tuned_model_endpoint_name,\n",
        "            'error': None\n",
        "        }\n",
        "        print(f\"‚úÖ Fine-tuning: SUPPORTED\")\n",
        "        print(f\"   Endpoint: {tuning_job.tuned_model_endpoint_name}\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        fine_tuning_results[model_name] = {\n",
        "            'status': '‚ùå NOT SUPPORTED',\n",
        "            'endpoint': None,\n",
        "            'error': str(e)\n",
        "        }\n",
        "        print(f\"‚ùå Fine-tuning: NOT SUPPORTED - {str(e)[:100]}...\")\n",
        "\n",
        "# Summary Report\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"üìä TESTING SUMMARY REPORT\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "print(\"\\nüîπ INFERENCE RESULTS:\")\n",
        "print(\"-\" * 30)\n",
        "for model, result in inference_results.items():\n",
        "    status_emoji = \"‚úÖ\" if result['status'].startswith(\"‚úÖ\") else \"‚ùå\"\n",
        "    print(f\"{status_emoji} {model}: {result['status']}\")\n",
        "\n",
        "print(\"\\nüîπ FINE-TUNING RESULTS:\")\n",
        "print(\"-\" * 30)\n",
        "for model, result in fine_tuning_results.items():\n",
        "    status_emoji = \"‚úÖ\" if result['status'].startswith(\"‚úÖ\") else \"‚ùå\"\n",
        "    print(f\"{status_emoji} {model}: {result['status']}\")\n",
        "\n",
        "# Recommendations\n",
        "print(\"\\nüéØ RECOMMENDATIONS:\")\n",
        "print(\"-\" * 20)\n",
        "working_models = [m for m, r in inference_results.items() if r['status'].startswith(\"‚úÖ\")]\n",
        "fine_tuning_models = [m for m, r in fine_tuning_results.items() if r['status'].startswith(\"‚úÖ\")]\n",
        "\n",
        "if working_models:\n",
        "    print(f\"‚úÖ Use these models for inference: {', '.join(working_models[:3])}\")\n",
        "else:\n",
        "    print(\"‚ùå No models working for inference\")\n",
        "\n",
        "if fine_tuning_models:\n",
        "    print(f\"üîß Use these models for fine-tuning: {', '.join(fine_tuning_models)}\")\n",
        "else:\n",
        "    print(\"‚ùå No models support fine-tuning in this region/project\")\n",
        "    print(\"üí° Try using AI Studio API instead of Vertex AI for fine-tuning\")\n",
        "\n",
        "print(f\"\\nüìà Total tested: {len(MODELS_TO_TEST)} models\")\n",
        "print(f\"‚úÖ Inference working: {len(working_models)} models\")\n",
        "print(f\"üîß Fine-tuning supported: {len(fine_tuning_models)} models\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "9db6508f",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚ö†Ô∏è No tuned model endpoint found\n",
            "üí° Steps:\n",
            "   1. Make sure fine-tuning cell completed successfully\n",
            "   2. Check Cloud Console for the tuned model endpoint\n",
            "   3. Set: TUNED_MODEL_ENDPOINT = 'your-endpoint-here'\n",
            "   4. Re-run this cell\n"
          ]
        }
      ],
      "source": [
        "# üß™ Test the tuned model (run this after fine-tuning completes)\n",
        "\n",
        "# Check if we have the tuned model endpoint\n",
        "if 'TUNED_MODEL_ENDPOINT' in globals() and TUNED_MODEL_ENDPOINT:\n",
        "    print(f\"üß™ Testing tuned model: {TUNED_MODEL_ENDPOINT}\")\n",
        "    \n",
        "    try:\n",
        "        tuned_model = GenerativeModel(TUNED_MODEL_ENDPOINT)\n",
        "        \n",
        "        # Test questions\n",
        "        test_questions = [\n",
        "            'What is a large language model?',\n",
        "            'What is machine learning?',\n",
        "            'Explain fine-tuning in AI.'\n",
        "        ]\n",
        "        \n",
        "        for i, question in enumerate(test_questions, 1):\n",
        "            print(f\"\\n‚ùì Question {i}: {question}\")\n",
        "            response = tuned_model.generate_content(question)\n",
        "            print(f\"üí¨ Answer: {response.text}\")\n",
        "            print(\"-\" * 50)\n",
        "        \n",
        "        print(\"\\n‚úÖ Tuned model is working perfectly!\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error testing tuned model: {e}\")\n",
        "        print(f\"üí° Make sure fine-tuning completed successfully\")\n",
        "        \n",
        "elif 'TUNED_MODEL_NAME' in globals() and TUNED_MODEL_NAME:\n",
        "    print(f\"üß™ Testing tuned model: {TUNED_MODEL_NAME}\")\n",
        "    \n",
        "    try:\n",
        "        tuned_model = GenerativeModel(TUNED_MODEL_NAME)\n",
        "        response = tuned_model.generate_content('What is a large language model?')\n",
        "        print(f\"üí¨ Response: {response.text}\")\n",
        "        print(\"\\n‚úÖ Tuned model is working!\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error: {e}\")\n",
        "        \n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No tuned model endpoint found\")\n",
        "    print(\"üí° Steps:\")\n",
        "    print(\"   1. Make sure fine-tuning cell completed successfully\")\n",
        "    print(\"   2. Check Cloud Console for the tuned model endpoint\")\n",
        "    print(\"   3. Set: TUNED_MODEL_ENDPOINT = 'your-endpoint-here'\")\n",
        "    print(\"   4. Re-run this cell\")\n",
        "    \n",
        "    # You can manually set the endpoint here:\n",
        "    # TUNED_MODEL_ENDPOINT = \"projects/.../locations/.../endpoints/...\"\n",
        "    # tuned_model = GenerativeModel(TUNED_MODEL_ENDPOINT)\n",
        "    # response = tuned_model.generate_content('What is a large language model?')\n",
        "    # print(response.text)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1117914a",
      "metadata": {},
      "source": [
        "Snippet 2. Using Vertex AI and Google AI studio SDKs for unimodal text generation "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "c8fee6a9",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        }
      ],
      "source": [
        "%pip install --upgrade --quiet google-genai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "5d70fc80",
      "metadata": {},
      "outputs": [
        {
          "ename": "ValueError",
          "evalue": "Project and location or API key must be set when using the Vertex AI API.",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 30\u001b[39m\n\u001b[32m     28\u001b[39m     PROJECT_ID = \u001b[38;5;28mstr\u001b[39m(os.environ.get(PROJECT_ID)) \n\u001b[32m     29\u001b[39m LOCATION = os.environ.get(REGION)  \n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m client = \u001b[43mgenai\u001b[49m\u001b[43m.\u001b[49m\u001b[43mClient\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvertexai\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mproject\u001b[49m\u001b[43m=\u001b[49m\u001b[43mPROJECT_ID\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m=\u001b[49m\u001b[43mLOCATION\u001b[49m\u001b[43m)\u001b[49m \n\u001b[32m     31\u001b[39m MODEL_ID = \u001b[33m\"\u001b[39m\u001b[33mgemini-2.0-flash-001\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;66;03m# @param {type: ‚Äústring‚Äù} \u001b[39;00m\n\u001b[32m     32\u001b[39m response = client.models.generate_content(\n\u001b[32m     33\u001b[39m     model=MODEL_ID, contents=\u001b[33m\"\u001b[39m\u001b[33mWhat‚Äôs the largest planet in our solar system?\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     34\u001b[39m )\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.11/site-packages/google/genai/client.py:265\u001b[39m, in \u001b[36mClient.__init__\u001b[39m\u001b[34m(self, vertexai, api_key, credentials, project, location, debug_config, http_options)\u001b[39m\n\u001b[32m    262\u001b[39m   \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    263\u001b[39m     http_options = HttpOptions(base_url=base_url)\n\u001b[32m--> \u001b[39m\u001b[32m265\u001b[39m \u001b[38;5;28mself\u001b[39m._api_client = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_get_api_client\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    266\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvertexai\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvertexai\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    267\u001b[39m \u001b[43m    \u001b[49m\u001b[43mapi_key\u001b[49m\u001b[43m=\u001b[49m\u001b[43mapi_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    268\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcredentials\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcredentials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    269\u001b[39m \u001b[43m    \u001b[49m\u001b[43mproject\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproject\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    270\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    271\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdebug_config\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_debug_config\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    272\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhttp_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhttp_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    273\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    275\u001b[39m \u001b[38;5;28mself\u001b[39m._aio = AsyncClient(\u001b[38;5;28mself\u001b[39m._api_client)\n\u001b[32m    276\u001b[39m \u001b[38;5;28mself\u001b[39m._models = Models(\u001b[38;5;28mself\u001b[39m._api_client)\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.11/site-packages/google/genai/client.py:311\u001b[39m, in \u001b[36mClient._get_api_client\u001b[39m\u001b[34m(vertexai, api_key, credentials, project, location, debug_config, http_options)\u001b[39m\n\u001b[32m    294\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m debug_config \u001b[38;5;129;01mand\u001b[39;00m debug_config.client_mode \u001b[38;5;129;01min\u001b[39;00m [\n\u001b[32m    295\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mrecord\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m    296\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mreplay\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m    297\u001b[39m     \u001b[33m'\u001b[39m\u001b[33mauto\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m    298\u001b[39m ]:\n\u001b[32m    299\u001b[39m   \u001b[38;5;28;01mreturn\u001b[39;00m ReplayApiClient(\n\u001b[32m    300\u001b[39m       mode=debug_config.client_mode,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[32m    301\u001b[39m       replay_id=debug_config.replay_id,  \u001b[38;5;66;03m# type: ignore[arg-type]\u001b[39;00m\n\u001b[32m   (...)\u001b[39m\u001b[32m    308\u001b[39m       http_options=http_options,\n\u001b[32m    309\u001b[39m   )\n\u001b[32m--> \u001b[39m\u001b[32m311\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mBaseApiClient\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    312\u001b[39m \u001b[43m    \u001b[49m\u001b[43mvertexai\u001b[49m\u001b[43m=\u001b[49m\u001b[43mvertexai\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    313\u001b[39m \u001b[43m    \u001b[49m\u001b[43mapi_key\u001b[49m\u001b[43m=\u001b[49m\u001b[43mapi_key\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    314\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcredentials\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcredentials\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    315\u001b[39m \u001b[43m    \u001b[49m\u001b[43mproject\u001b[49m\u001b[43m=\u001b[49m\u001b[43mproject\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    316\u001b[39m \u001b[43m    \u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlocation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    317\u001b[39m \u001b[43m    \u001b[49m\u001b[43mhttp_options\u001b[49m\u001b[43m=\u001b[49m\u001b[43mhttp_options\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    318\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[36mFile \u001b[39m\u001b[32m~/.local/lib/python3.11/site-packages/google/genai/_api_client.py:644\u001b[39m, in \u001b[36mBaseApiClient.__init__\u001b[39m\u001b[34m(self, vertexai, api_key, credentials, project, location, http_options)\u001b[39m\n\u001b[32m    640\u001b[39m has_sufficient_auth = (\u001b[38;5;28mself\u001b[39m.project \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28mself\u001b[39m.location) \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.api_key\n\u001b[32m    642\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m has_sufficient_auth \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m validated_http_options.base_url:\n\u001b[32m    643\u001b[39m   \u001b[38;5;66;03m# Skip sufficient auth check if base url is provided in http options.\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m644\u001b[39m   \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    645\u001b[39m       \u001b[33m'\u001b[39m\u001b[33mProject and location or API key must be set when using the Vertex \u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    646\u001b[39m       \u001b[33m'\u001b[39m\u001b[33mAI API.\u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    647\u001b[39m   )\n\u001b[32m    648\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m.api_key \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m.location == \u001b[33m'\u001b[39m\u001b[33mglobal\u001b[39m\u001b[33m'\u001b[39m:\n\u001b[32m    649\u001b[39m   \u001b[38;5;28mself\u001b[39m._http_options.base_url = \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mhttps://aiplatform.googleapis.com/\u001b[39m\u001b[33m'\u001b[39m\n",
            "\u001b[31mValueError\u001b[39m: Project and location or API key must be set when using the Vertex AI API."
          ]
        }
      ],
      "source": [
        "import sys \n",
        "if \"google.colab\" in sys.modules:\n",
        "    from google.colab import auth  \n",
        "    auth.authenticate_user() \n",
        "from IPython.display import HTML, Markdown, display\n",
        "from google import genai\n",
        "from google.genai.types import (\n",
        "    FunctionDeclaration,\n",
        "    GenerateContentConfig,\n",
        "    GoogleSearch,\n",
        "    HarmBlockThreshold,\n",
        "    HarmCategory,\n",
        "    MediaResolution,\n",
        "    Part,\n",
        "    Retrieval,\n",
        "    SafetySetting,\n",
        "    Tool,\n",
        "    ToolCodeExecution,\n",
        "    VertexAISearch,\n",
        ")\n",
        "import os\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "\n",
        "PROJECT_ID = os.getenv('PROJECT_ID')  # @param {type: ‚Äústring‚Äù, placeholder: ‚Äú[your-project-id]‚Äù, isTemplate: true}\n",
        "if not PROJECT_ID or PROJECT_ID == PROJECT_ID:\n",
        "    PROJECT_ID = str(os.environ.get(PROJECT_ID)) \n",
        "LOCATION = os.environ.get(REGION)  \n",
        "client = genai.Client(vertexai=True, project=PROJECT_ID, location=LOCATION) \n",
        "MODEL_ID = \"gemini-2.0-flash-001\" # @param {type: ‚Äústring‚Äù} \n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID, contents=\"What‚Äôs the largest planet in our solar system?\"\n",
        ")\n",
        "display(Markdown(response.text))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Codecademy ML",
      "language": "python",
      "name": "codeacademy"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
