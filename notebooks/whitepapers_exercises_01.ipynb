{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "bd3e7092",
      "metadata": {},
      "source": [
        "## Vertex AI Gemini Fine-Tuning (PEFT) Quickstart\n",
        "\n",
        "Based on the Feb 2025 Vertex AI documentation (\"Tune Gemini models with adapters\"), supervised\n",
        "fine-tuning currently supports Gemini 1.0 Pro and the preview Gemini 1.5 adapter variants in\n",
        "`us-central1`. This notebook follows that flow and replaces earlier cells that tried to fine-tune\n",
        "unsupported model identifiers.\n",
        "\n",
        "Before running the code cells:\n",
        "- Enable billing for your Google Cloud project.\n",
        "- Export `GOOGLE_CLOUD_PROJECT` (and optionally `GOOGLE_CLOUD_REGION`) or edit the constants in the\n",
        "  configuration cell.\n",
        "- Restart the runtime after any package upgrades.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "id": "2553aab7",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Project: periospot-mvp\n",
            "Region: us-central1\n",
            "google-cloud-aiplatform: 1.119.0\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "from datetime import datetime\n",
        "\n",
        "try:\n",
        "    from dotenv import load_dotenv\n",
        "except ModuleNotFoundError:\n",
        "    def load_dotenv():\n",
        "        return None\n",
        "\n",
        "import google.cloud.aiplatform as aiplatform\n",
        "import vertexai\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "PROJECT_ID = os.environ.get('GOOGLE_CLOUD_PROJECT') or os.environ.get('PROJECT_ID') or 'periospot-mvp'\n",
        "REGION = os.environ.get('GOOGLE_CLOUD_REGION', 'us-central1')\n",
        "\n",
        "if PROJECT_ID in {'', 'your-project-id', '<project_id>'}:\n",
        "    raise ValueError('Set PROJECT_ID or export GOOGLE_CLOUD_PROJECT before continuing.')\n",
        "\n",
        "if REGION != 'us-central1':\n",
        "    raise ValueError(\"Vertex AI Gemini supervised fine-tuning currently runs only in 'us-central1'.\")\n",
        "\n",
        "vertexai.init(project=PROJECT_ID, location=REGION)\n",
        "\n",
        "print(f'Project: {PROJECT_ID}')\n",
        "print(f'Region: {REGION}')\n",
        "print(f'google-cloud-aiplatform: {aiplatform.__version__}')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "id": "38479259",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üîß Creating properly formatted training dataset...\n",
            "‚úÖ Created training file: /var/folders/cd/2c6x3jgj47j_fwl7231ts7m00000gn/T/tmpha9mjqre/training_data.jsonl\n",
            "\n",
            "üì§ Uploading training data to GCS...\n",
            "‚úÖ Using existing bucket: periospot-mvp-gemini-tuning\n",
            "‚úÖ Uploaded training data to: gs://periospot-mvp-gemini-tuning/training_data/training_20251007_152358.jsonl\n",
            "\n",
            "‚úÖ Configuration:\n",
            "Base model: gemini-2.5-pro (GA)\n",
            "Training dataset: gs://periospot-mvp-gemini-tuning/training_data/training_20251007_152358.jsonl\n",
            "Tuned model display name: gemini-peft-20251007-152359\n",
            "\n",
            "üéâ Ready for fine-tuning with properly formatted dataset!\n"
          ]
        }
      ],
      "source": [
        "from vertexai.generative_models import GenerativeModel\n",
        "from vertexai.preview.tuning import sft\n",
        "import json\n",
        "from google.cloud import storage\n",
        "\n",
        "SUPPORTED_BASE_MODELS = {\n",
        "    'gemini-2.5-pro': 'GA',\n",
        "    'gemini-2.5-flash': 'GA',\n",
        "    'gemini-2.5-flash-lite': 'GA',\n",
        "    'gemini-1.0-pro-002': 'GA',\n",
        "    'gemini-1.5-pro-002': 'Preview',\n",
        "    'gemini-1.5-flash-002': 'Preview',\n",
        "    'text-bison@002': 'Legacy',\n",
        "}\n",
        "\n",
        "# ‚ö†Ô∏è THE GOOGLE SAMPLE DATASET HAS WRONG FORMAT!\n",
        "# Old format: {\"input_text\": \"...\", \"output_text\": \"...\"}\n",
        "# New format for Gemini 2.5: {\"contents\": [{\"role\": \"user\", \"parts\": [{\"text\": \"...\"}]}, ...]}\n",
        "\n",
        "# Let's create a properly formatted training dataset\n",
        "print(\"üîß Creating properly formatted training dataset...\")\n",
        "\n",
        "# Sample training data in CORRECT format for Gemini 2.5\n",
        "training_examples = [\n",
        "    {\n",
        "        \"contents\": [\n",
        "            {\"role\": \"user\", \"parts\": [{\"text\": \"What is machine learning?\"}]},\n",
        "            {\"role\": \"model\", \"parts\": [{\"text\": \"Machine learning is a subset of artificial intelligence (AI) that enables computers to learn and improve from experience without being explicitly programmed. It uses algorithms and statistical models to analyze and draw inferences from patterns in data.\"}]}\n",
        "        ]\n",
        "    },\n",
        "    {\n",
        "        \"contents\": [\n",
        "            {\"role\": \"user\", \"parts\": [{\"text\": \"Explain neural networks in simple terms.\"}]},\n",
        "            {\"role\": \"model\", \"parts\": [{\"text\": \"Neural networks are computing systems inspired by the biological neural networks in animal brains. They consist of interconnected nodes (neurons) organized in layers that process information. These networks can learn to perform tasks by analyzing examples, gradually improving their accuracy through a process called training.\"}]}\n",
        "        ]\n",
        "    },\n",
        "    {\n",
        "        \"contents\": [\n",
        "            {\"role\": \"user\", \"parts\": [{\"text\": \"What is fine-tuning in AI?\"}]},\n",
        "            {\"role\": \"model\", \"parts\": [{\"text\": \"Fine-tuning is a process where you take a pre-trained AI model and train it further on a specific dataset relevant to your task. This allows the model to adapt its knowledge to perform better on particular applications while retaining the general capabilities it learned during initial training.\"}]}\n",
        "        ]\n",
        "    },\n",
        "]\n",
        "\n",
        "# Save to a local file\n",
        "import tempfile\n",
        "import os\n",
        "\n",
        "# Create temp directory for training data\n",
        "temp_dir = tempfile.mkdtemp()\n",
        "local_training_file = os.path.join(temp_dir, 'training_data.jsonl')\n",
        "\n",
        "with open(local_training_file, 'w') as f:\n",
        "    for example in training_examples:\n",
        "        f.write(json.dumps(example) + '\\n')\n",
        "\n",
        "print(f\"‚úÖ Created training file: {local_training_file}\")\n",
        "\n",
        "# ‚ö†Ô∏è MUST upload to GCS - local files don't work!\n",
        "print(\"\\nüì§ Uploading training data to GCS...\")\n",
        "\n",
        "# Create GCS bucket name (must be globally unique)\n",
        "BUCKET_NAME = f'{PROJECT_ID}-gemini-tuning'\n",
        "GCS_TRAINING_PATH = f'gs://{BUCKET_NAME}/training_data/training_{datetime.utcnow():%Y%m%d_%H%M%S}.jsonl'\n",
        "\n",
        "try:\n",
        "    # Initialize storage client\n",
        "    storage_client = storage.Client(project=PROJECT_ID)\n",
        "    \n",
        "    # Create bucket if it doesn't exist\n",
        "    try:\n",
        "        bucket = storage_client.create_bucket(BUCKET_NAME, location='us-central1')\n",
        "        print(f\"‚úÖ Created bucket: {BUCKET_NAME}\")\n",
        "    except Exception as e:\n",
        "        if 'already exists' in str(e).lower() or '409' in str(e):\n",
        "            bucket = storage_client.bucket(BUCKET_NAME)\n",
        "            print(f\"‚úÖ Using existing bucket: {BUCKET_NAME}\")\n",
        "        else:\n",
        "            raise\n",
        "    \n",
        "    # Upload the training file\n",
        "    blob_name = GCS_TRAINING_PATH.replace(f'gs://{BUCKET_NAME}/', '')\n",
        "    blob = bucket.blob(blob_name)\n",
        "    blob.upload_from_filename(local_training_file)\n",
        "    \n",
        "    print(f\"‚úÖ Uploaded training data to: {GCS_TRAINING_PATH}\")\n",
        "    \n",
        "    TRAINING_DATASET = GCS_TRAINING_PATH\n",
        "    VALIDATION_DATASET = None\n",
        "    \n",
        "except Exception as e:\n",
        "    print(f\"‚ùå Error uploading to GCS: {e}\")\n",
        "    print(\"\\nüí° ALTERNATIVE: Create your bucket manually:\")\n",
        "    print(f\"   1. Go to https://console.cloud.google.com/storage\")\n",
        "    print(f\"   2. Create bucket: {BUCKET_NAME}\")\n",
        "    print(f\"   3. Upload {local_training_file} to the bucket\")\n",
        "    print(f\"   4. Update TRAINING_DATASET = 'gs://{BUCKET_NAME}/your_file.jsonl'\")\n",
        "    raise\n",
        "\n",
        "BASE_MODEL = 'gemini-2.5-pro'\n",
        "TUNED_MODEL_DISPLAY_NAME = f'gemini-peft-{datetime.utcnow():%Y%m%d-%H%M%S}'\n",
        "\n",
        "if BASE_MODEL not in SUPPORTED_BASE_MODELS:\n",
        "    raise ValueError(f'Unsupported base model {BASE_MODEL}. Choose one of {list(SUPPORTED_BASE_MODELS)}.')\n",
        "\n",
        "print(f'\\n‚úÖ Configuration:')\n",
        "print(f'Base model: {BASE_MODEL} ({SUPPORTED_BASE_MODELS[BASE_MODEL]})')\n",
        "print(f'Training dataset: {TRAINING_DATASET}')\n",
        "print(f'Tuned model display name: {TUNED_MODEL_DISPLAY_NAME}')\n",
        "print(f'\\nüéâ Ready for fine-tuning with properly formatted dataset!')\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "aeefd555",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üöÄ Starting fine-tuning job...\n",
            "Creating SupervisedTuningJob\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
            "E0000 00:00:1759850639.604962 16652295 alts_credentials.cc:93] ALTS creds ignored. Not running on GCP and untrusted ALTS is not enabled.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "SupervisedTuningJob created. Resource name: projects/1020593801304/locations/us-central1/tuningJobs/1542662865863835648\n",
            "To use this SupervisedTuningJob in another session:\n",
            "tuning_job = sft.SupervisedTuningJob('projects/1020593801304/locations/us-central1/tuningJobs/1542662865863835648')\n",
            "View Tuning Job:\n",
            "https://console.cloud.google.com/vertex-ai/generative/language/locations/us-central1/tuning/tuningJob/1542662865863835648?project=1020593801304\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "        \n",
              "    <link rel=\"stylesheet\" href=\"https://fonts.googleapis.com/icon?family=Material+Icons\">\n",
              "    <style>\n",
              "      .view-vertex-resource,\n",
              "      .view-vertex-resource:hover,\n",
              "      .view-vertex-resource:visited {\n",
              "        position: relative;\n",
              "        display: inline-flex;\n",
              "        flex-direction: row;\n",
              "        height: 32px;\n",
              "        padding: 0 12px;\n",
              "          margin: 4px 18px;\n",
              "        gap: 4px;\n",
              "        border-radius: 4px;\n",
              "\n",
              "        align-items: center;\n",
              "        justify-content: center;\n",
              "        background-color: rgb(255, 255, 255);\n",
              "        color: rgb(51, 103, 214);\n",
              "\n",
              "        font-family: Roboto,\"Helvetica Neue\",sans-serif;\n",
              "        font-size: 13px;\n",
              "        font-weight: 500;\n",
              "        text-transform: uppercase;\n",
              "        text-decoration: none !important;\n",
              "\n",
              "        transition: box-shadow 280ms cubic-bezier(0.4, 0, 0.2, 1) 0s;\n",
              "        box-shadow: 0px 3px 1px -2px rgba(0,0,0,0.2), 0px 2px 2px 0px rgba(0,0,0,0.14), 0px 1px 5px 0px rgba(0,0,0,0.12);\n",
              "      }\n",
              "      .view-vertex-resource:active {\n",
              "        box-shadow: 0px 5px 5px -3px rgba(0,0,0,0.2),0px 8px 10px 1px rgba(0,0,0,0.14),0px 3px 14px 2px rgba(0,0,0,0.12);\n",
              "      }\n",
              "      .view-vertex-resource:active .view-vertex-ripple::before {\n",
              "        position: absolute;\n",
              "        top: 0;\n",
              "        bottom: 0;\n",
              "        left: 0;\n",
              "        right: 0;\n",
              "        border-radius: 4px;\n",
              "        pointer-events: none;\n",
              "\n",
              "        content: '';\n",
              "        background-color: rgb(51, 103, 214);\n",
              "        opacity: 0.12;\n",
              "      }\n",
              "      .view-vertex-icon {\n",
              "        font-size: 18px;\n",
              "      }\n",
              "    </style>\n",
              "  \n",
              "        <a class=\"view-vertex-resource\" id=\"view-vertex-resource-6a58c5a8-eff5-4a3a-a2ed-9a274bb5fbe8\" href=\"#view-view-vertex-resource-6a58c5a8-eff5-4a3a-a2ed-9a274bb5fbe8\">\n",
              "          <span class=\"material-icons view-vertex-icon\">tune</span>\n",
              "          <span>View Tuning Job</span>\n",
              "        </a>\n",
              "        \n",
              "        <script>\n",
              "          (function () {\n",
              "            const link = document.getElementById('view-vertex-resource-6a58c5a8-eff5-4a3a-a2ed-9a274bb5fbe8');\n",
              "            link.addEventListener('click', (e) => {\n",
              "              if (window.google?.colab?.openUrl) {\n",
              "                window.google.colab.openUrl('https://console.cloud.google.com/vertex-ai/generative/language/locations/us-central1/tuning/tuningJob/1542662865863835648?project=1020593801304');\n",
              "              } else {\n",
              "                window.open('https://console.cloud.google.com/vertex-ai/generative/language/locations/us-central1/tuning/tuningJob/1542662865863835648?project=1020593801304', '_blank');\n",
              "              }\n",
              "              e.stopPropagation();\n",
              "              e.preventDefault();\n",
              "            });\n",
              "          })();\n",
              "        </script>\n",
              "    "
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Started tuning job: projects/1020593801304/locations/us-central1/tuningJobs/1542662865863835648\n",
            "üìä Job name: 1542662865863835648\n",
            "‚è∞ This may take 15-30 minutes to complete.\n",
            "\n",
            "üîó Monitor progress at:\n",
            "   https://console.cloud.google.com/vertex-ai/locations/us-central1/tuning-jobs/1542662865863835648?project=periospot-mvp\n",
            "\n",
            "‚è≥ Monitoring job status (this will wait until completion)...\n",
            "‚è±Ô∏è Status: JOB_STATE_PENDING\n",
            "‚è±Ô∏è Status: JOB_STATE_PENDING\n",
            "‚è±Ô∏è Status: JOB_STATE_PENDING\n",
            "‚è±Ô∏è Status: JOB_STATE_PENDING\n",
            "‚è±Ô∏è Status: JOB_STATE_PENDING\n",
            "‚è±Ô∏è Status: JOB_STATE_PENDING\n",
            "‚è±Ô∏è Status: JOB_STATE_PENDING\n",
            "‚è±Ô∏è Status: JOB_STATE_PENDING\n",
            "‚è±Ô∏è Status: JOB_STATE_PENDING\n",
            "‚è±Ô∏è Status: JOB_STATE_PENDING\n"
          ]
        }
      ],
      "source": [
        "train_kwargs = {\n",
        "    'source_model': BASE_MODEL,\n",
        "    'train_dataset': TRAINING_DATASET,\n",
        "    'tuned_model_display_name': TUNED_MODEL_DISPLAY_NAME\n",
        "}\n",
        "if VALIDATION_DATASET:\n",
        "    train_kwargs['validation_dataset'] = VALIDATION_DATASET\n",
        "\n",
        "print(\"üöÄ Starting fine-tuning job...\")\n",
        "tuning_job = sft.train(**train_kwargs)\n",
        "\n",
        "print(f'‚úÖ Started tuning job: {tuning_job.resource_name}')\n",
        "print(f'üìä Job name: {tuning_job.name}')\n",
        "print(f'‚è∞ This may take 15-30 minutes to complete.')\n",
        "\n",
        "# The job runs asynchronously - you can check status in the Cloud Console\n",
        "# URL: https://console.cloud.google.com/vertex-ai/generative/language/tuning-jobs\n",
        "print(f'\\nüîó Monitor progress at:')\n",
        "print(f'   https://console.cloud.google.com/vertex-ai/locations/{REGION}/tuning-jobs/{tuning_job.name.split(\"/\")[-1]}?project={PROJECT_ID}')\n",
        "\n",
        "# For monitoring in the notebook, you can poll the status\n",
        "print(f'\\n‚è≥ Monitoring job status (this will wait until completion)...')\n",
        "import time\n",
        "\n",
        "# Poll every 30 seconds\n",
        "max_wait_time = 3600  # 1 hour max\n",
        "start_time = time.time()\n",
        "check_interval = 30  # seconds\n",
        "\n",
        "while True:\n",
        "    # Refresh job status\n",
        "    try:\n",
        "        # Get current state\n",
        "        current_state = tuning_job.state\n",
        "        print(f\"‚è±Ô∏è Status: {current_state.name if hasattr(current_state, 'name') else current_state}\")\n",
        "        \n",
        "        # Check if job is done\n",
        "        if hasattr(current_state, 'name'):\n",
        "            state_name = current_state.name\n",
        "        else:\n",
        "            state_name = str(current_state)\n",
        "        \n",
        "        if 'SUCCEEDED' in state_name or 'JOB_STATE_SUCCEEDED' in state_name:\n",
        "            print(\"üéâ Fine-tuning completed successfully!\")\n",
        "            break\n",
        "        elif 'FAILED' in state_name or 'CANCELLED' in state_name:\n",
        "            print(f\"‚ùå Fine-tuning failed with state: {state_name}\")\n",
        "            break\n",
        "        \n",
        "        # Check timeout\n",
        "        elapsed = time.time() - start_time\n",
        "        if elapsed > max_wait_time:\n",
        "            print(f\"‚è∞ Timeout after {max_wait_time/60:.1f} minutes\")\n",
        "            print(\"üí° Job is still running. Check Cloud Console for status.\")\n",
        "            break\n",
        "        \n",
        "        # Wait before next check\n",
        "        time.sleep(check_interval)\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"‚ö†Ô∏è Error checking status: {e}\")\n",
        "        print(\"üí° Check Cloud Console for job status\")\n",
        "        break\n",
        "\n",
        "# Get the tuned model information\n",
        "try:\n",
        "    if hasattr(tuning_job, 'tuned_model_name') and tuning_job.tuned_model_name:\n",
        "        TUNED_MODEL_NAME = tuning_job.tuned_model_name\n",
        "        print(f\"\\n‚úÖ Tuned model name: {TUNED_MODEL_NAME}\")\n",
        "    \n",
        "    if hasattr(tuning_job, 'tuned_model_endpoint_name') and tuning_job.tuned_model_endpoint_name:\n",
        "        TUNED_MODEL_ENDPOINT = tuning_job.tuned_model_endpoint_name\n",
        "        print(f\"‚úÖ Tuned model endpoint: {TUNED_MODEL_ENDPOINT}\")\n",
        "    else:\n",
        "        print(f\"\\n‚ö†Ô∏è Model endpoint not yet available\")\n",
        "        print(f\"üí° Check the Cloud Console URL above for the endpoint once training completes\")\n",
        "        \n",
        "except Exception as e:\n",
        "    print(f\"‚ö†Ô∏è Could not retrieve tuned model info: {e}\")\n",
        "    print(f\"üí° You can get the endpoint from Cloud Console once training completes\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "0342e8f8",
      "metadata": {},
      "source": [
        "## üéâ **SOLUTION**: Correctly Formatted Training Dataset\n",
        "\n",
        "### ‚ö†Ô∏è **Root Cause of Error**\n",
        "The error `\"Row: 0. Missing required 'contents' field\"` was caused by using Google's sample dataset which has an **OUTDATED FORMAT**.\n",
        "\n",
        "### ‚úÖ **Correct Format for Gemini 2.5**\n",
        "Each training example must use this structure:\n",
        "```json\n",
        "{\n",
        "  \"contents\": [\n",
        "    {\"role\": \"user\", \"parts\": [{\"text\": \"Your question here\"}]},\n",
        "    {\"role\": \"model\", \"parts\": [{\"text\": \"Expected response here\"}]}\n",
        "  ]\n",
        "}\n",
        "```\n",
        "\n",
        "The cell above creates a properly formatted dataset that will work with Gemini 2.5 fine-tuning!\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1b9f00c1",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "üöÄ COMPREHENSIVE MODEL TESTING\n",
            "==================================================\n",
            "\n",
            "üîç Testing model: gemini-2.5-pro\n",
            "----------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "E0000 00:00:1759642755.007235 1550705 alts_credentials.cc:93] ALTS creds ignored. Not running on GCP and untrusted ALTS is not enabled.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Inference: SUCCESS (27.16s)\n",
            "   Preview: Of course! Here‚Äôs a detailed breakdown of what an LLM is, explained in simple te...\n",
            "üîß Testing fine-tuning...\n",
            "Creating SupervisedTuningJob\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "E0000 00:00:1759642782.141396 1550705 alts_credentials.cc:93] ALTS creds ignored. Not running on GCP and untrusted ALTS is not enabled.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚ùå Fine-tuning: NOT SUPPORTED - 400 Row: 0. Missing required `contents` field....\n",
            "\n",
            "üîç Testing model: gemini-2.5-flash\n",
            "----------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "E0000 00:00:1759642783.172089 1550705 alts_credentials.cc:93] ALTS creds ignored. Not running on GCP and untrusted ALTS is not enabled.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Inference: SUCCESS (9.26s)\n",
            "   Preview: An **LLM** stands for **Large Language Model**.\n",
            "\n",
            "In essence, an LLM is a type of...\n",
            "üîß Testing fine-tuning...\n",
            "Creating SupervisedTuningJob\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "E0000 00:00:1759642792.428194 1550705 alts_credentials.cc:93] ALTS creds ignored. Not running on GCP and untrusted ALTS is not enabled.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚ùå Fine-tuning: NOT SUPPORTED - 400 Row: 0. Missing required `contents` field....\n",
            "\n",
            "üîç Testing model: gemini-2.5-flash-lite\n",
            "----------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "E0000 00:00:1759642793.959865 1550705 alts_credentials.cc:93] ALTS creds ignored. Not running on GCP and untrusted ALTS is not enabled.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Inference: SUCCESS (1.93s)\n",
            "   Preview: LLM stands for **Large Language Model**.\n",
            "\n",
            "In simple terms, it's a type of **arti...\n",
            "üîß Testing fine-tuning...\n",
            "Creating SupervisedTuningJob\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "E0000 00:00:1759642795.893665 1550705 alts_credentials.cc:93] ALTS creds ignored. Not running on GCP and untrusted ALTS is not enabled.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚ùå Fine-tuning: NOT SUPPORTED - 400 Row: 0. Missing required `contents` field....\n",
            "\n",
            "üîç Testing model: gemini-2.0-flash\n",
            "----------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "E0000 00:00:1759642796.835614 1550705 alts_credentials.cc:93] ALTS creds ignored. Not running on GCP and untrusted ALTS is not enabled.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚úÖ Inference: SUCCESS (4.49s)\n",
            "   Preview: LLM stands for **Large Language Model**.  It's a type of artificial intelligence...\n",
            "üîß Testing fine-tuning...\n",
            "Creating SupervisedTuningJob\n",
            "‚ùå Fine-tuning: NOT SUPPORTED - 400 Base model gemini-2.0-flash is not supported....\n",
            "\n",
            "üîç Testing model: gemini-2.0-flash-exp\n",
            "----------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "E0000 00:00:1759642801.328409 1550705 alts_credentials.cc:93] ALTS creds ignored. Not running on GCP and untrusted ALTS is not enabled.\n",
            "E0000 00:00:1759642801.505201 1550705 alts_credentials.cc:93] ALTS creds ignored. Not running on GCP and untrusted ALTS is not enabled.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚ùå Inference: FAILED - 404 Publisher Model `projects/periospot-mvp/locations/europe-west1/publishers/google/models/gemini-2...\n",
            "\n",
            "üîç Testing model: gemini-2.0-flash-thinking-exp\n",
            "----------------------------------------\n",
            "‚ùå Inference: FAILED - 404 Publisher Model `projects/periospot-mvp/locations/europe-west1/publishers/google/models/gemini-2...\n",
            "\n",
            "üîç Testing model: gemini-1.5-flash\n",
            "----------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "E0000 00:00:1759642801.657061 1550705 alts_credentials.cc:93] ALTS creds ignored. Not running on GCP and untrusted ALTS is not enabled.\n",
            "E0000 00:00:1759642801.837641 1550705 alts_credentials.cc:93] ALTS creds ignored. Not running on GCP and untrusted ALTS is not enabled.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚ùå Inference: FAILED - 404 Publisher Model `projects/periospot-mvp/locations/europe-west1/publishers/google/models/gemini-1...\n",
            "\n",
            "üîç Testing model: gemini-1.5-flash-8b\n",
            "----------------------------------------\n",
            "‚ùå Inference: FAILED - 404 Publisher Model `projects/periospot-mvp/locations/europe-west1/publishers/google/models/gemini-1...\n",
            "\n",
            "üîç Testing model: gemini-1.5-pro\n",
            "----------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "E0000 00:00:1759642801.998568 1550705 alts_credentials.cc:93] ALTS creds ignored. Not running on GCP and untrusted ALTS is not enabled.\n",
            "E0000 00:00:1759642802.144981 1550705 alts_credentials.cc:93] ALTS creds ignored. Not running on GCP and untrusted ALTS is not enabled.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚ùå Inference: FAILED - 404 Publisher Model `projects/periospot-mvp/locations/europe-west1/publishers/google/models/gemini-1...\n",
            "\n",
            "üîç Testing model: gemini-1.5-pro-latest\n",
            "----------------------------------------\n",
            "‚ùå Inference: FAILED - 404 Publisher Model `projects/periospot-mvp/locations/europe-west1/publishers/google/models/gemini-1...\n",
            "\n",
            "üîç Testing model: gemini-1.0-pro\n",
            "----------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "E0000 00:00:1759642802.275764 1550705 alts_credentials.cc:93] ALTS creds ignored. Not running on GCP and untrusted ALTS is not enabled.\n",
            "E0000 00:00:1759642802.430768 1550705 alts_credentials.cc:93] ALTS creds ignored. Not running on GCP and untrusted ALTS is not enabled.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚ùå Inference: FAILED - 404 Publisher Model `projects/periospot-mvp/locations/europe-west1/publishers/google/models/gemini-1...\n",
            "\n",
            "üîç Testing model: gemini-1.0-pro-latest\n",
            "----------------------------------------\n",
            "‚ùå Inference: FAILED - 404 Publisher Model `projects/periospot-mvp/locations/europe-west1/publishers/google/models/gemini-1...\n",
            "\n",
            "üîç Testing model: gemini-exp-1206\n",
            "----------------------------------------\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "E0000 00:00:1759642802.554591 1550705 alts_credentials.cc:93] ALTS creds ignored. Not running on GCP and untrusted ALTS is not enabled.\n",
            "E0000 00:00:1759642802.707370 1550705 alts_credentials.cc:93] ALTS creds ignored. Not running on GCP and untrusted ALTS is not enabled.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "‚ùå Inference: FAILED - 404 Publisher Model `projects/periospot-mvp/locations/europe-west1/publishers/google/models/gemini-e...\n",
            "\n",
            "üîç Testing model: gemini-exp-1206-pro\n",
            "----------------------------------------\n",
            "‚ùå Inference: FAILED - 404 Publisher Model `projects/periospot-mvp/locations/europe-west1/publishers/google/models/gemini-e...\n",
            "\n",
            "============================================================\n",
            "üìä TESTING SUMMARY REPORT\n",
            "============================================================\n",
            "\n",
            "üîπ INFERENCE RESULTS:\n",
            "------------------------------\n",
            "‚úÖ gemini-2.5-pro: ‚úÖ SUCCESS\n",
            "‚úÖ gemini-2.5-flash: ‚úÖ SUCCESS\n",
            "‚úÖ gemini-2.5-flash-lite: ‚úÖ SUCCESS\n",
            "‚úÖ gemini-2.0-flash: ‚úÖ SUCCESS\n",
            "‚ùå gemini-2.0-flash-exp: ‚ùå FAILED\n",
            "‚ùå gemini-2.0-flash-thinking-exp: ‚ùå FAILED\n",
            "‚ùå gemini-1.5-flash: ‚ùå FAILED\n",
            "‚ùå gemini-1.5-flash-8b: ‚ùå FAILED\n",
            "‚ùå gemini-1.5-pro: ‚ùå FAILED\n",
            "‚ùå gemini-1.5-pro-latest: ‚ùå FAILED\n",
            "‚ùå gemini-1.0-pro: ‚ùå FAILED\n",
            "‚ùå gemini-1.0-pro-latest: ‚ùå FAILED\n",
            "‚ùå gemini-exp-1206: ‚ùå FAILED\n",
            "‚ùå gemini-exp-1206-pro: ‚ùå FAILED\n",
            "\n",
            "üîπ FINE-TUNING RESULTS:\n",
            "------------------------------\n",
            "‚ùå gemini-2.5-pro: ‚ùå NOT SUPPORTED\n",
            "‚ùå gemini-2.5-flash: ‚ùå NOT SUPPORTED\n",
            "‚ùå gemini-2.5-flash-lite: ‚ùå NOT SUPPORTED\n",
            "‚ùå gemini-2.0-flash: ‚ùå NOT SUPPORTED\n",
            "\n",
            "üéØ RECOMMENDATIONS:\n",
            "--------------------\n",
            "‚úÖ Use these models for inference: gemini-2.5-pro, gemini-2.5-flash, gemini-2.5-flash-lite\n",
            "‚ùå No models support fine-tuning in this region/project\n",
            "üí° Try using AI Studio API instead of Vertex AI for fine-tuning\n",
            "\n",
            "üìà Total tested: 14 models\n",
            "‚úÖ Inference working: 4 models\n",
            "üîß Fine-tuning supported: 0 models\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "E0000 00:00:1759642802.851959 1550705 alts_credentials.cc:93] ALTS creds ignored. Not running on GCP and untrusted ALTS is not enabled.\n"
          ]
        }
      ],
      "source": [
        "# üîç COMPREHENSIVE MODEL TESTING SYSTEM\n",
        "# This cell tests which models work for inference and fine-tuning\n",
        "\n",
        "import vertexai\n",
        "from vertexai.generative_models import GenerativeModel\n",
        "from vertexai.preview.tuning import sft\n",
        "import time\n",
        "\n",
        "# Initialize Vertex AI\n",
        "PROJECT_ID = 'periospot-mvp'\n",
        "REGION = 'europe-west1'\n",
        "vertexai.init(project=PROJECT_ID, location=REGION)\n",
        "\n",
        "# List of models to test - UPDATED with correct Gemini 2.5 models\n",
        "MODELS_TO_TEST = [\n",
        "    # Gemini 2.5 models (SUPPORTED for fine-tuning)\n",
        "    'gemini-2.5-pro',\n",
        "    'gemini-2.5-flash',\n",
        "    'gemini-2.5-flash-lite',\n",
        "    \n",
        "    # Gemini 2.0 models (inference only)\n",
        "    'gemini-2.0-flash',\n",
        "    'gemini-2.0-flash-exp',\n",
        "    'gemini-2.0-flash-thinking-exp',\n",
        "    \n",
        "    # Gemini 1.5 models (legacy, may not be available)\n",
        "    'gemini-1.5-flash',\n",
        "    'gemini-1.5-flash-8b',\n",
        "    'gemini-1.5-pro',\n",
        "    'gemini-1.5-pro-latest',\n",
        "    \n",
        "    # Gemini 1.0 models (legacy)\n",
        "    'gemini-1.0-pro',\n",
        "    'gemini-1.0-pro-latest',\n",
        "    \n",
        "    # Experimental models\n",
        "    'gemini-exp-1206',\n",
        "    'gemini-exp-1206-pro',\n",
        "]\n",
        "\n",
        "# Training dataset for fine-tuning tests\n",
        "TRAINING_DATASET = 'gs://cloud-samples-data/vertex-ai/model-evaluation/peft_train_sample.jsonl'\n",
        "\n",
        "print(\"üöÄ COMPREHENSIVE MODEL TESTING\")\n",
        "print(\"=\" * 50)\n",
        "\n",
        "# Results storage\n",
        "inference_results = {}\n",
        "fine_tuning_results = {}\n",
        "\n",
        "for model_name in MODELS_TO_TEST:\n",
        "    print(f\"\\nüîç Testing model: {model_name}\")\n",
        "    print(\"-\" * 40)\n",
        "    \n",
        "    # Test 1: Inference (generate_content)\n",
        "    try:\n",
        "        model = GenerativeModel(model_name)\n",
        "        start_time = time.time()\n",
        "        response = model.generate_content(\"What is a LLM?\")\n",
        "        end_time = time.time()\n",
        "        \n",
        "        inference_results[model_name] = {\n",
        "            'status': '‚úÖ SUCCESS',\n",
        "            'response_time': f\"{end_time - start_time:.2f}s\",\n",
        "            'response_preview': response.text[:100] + \"...\",\n",
        "            'error': None\n",
        "        }\n",
        "        print(f\"‚úÖ Inference: SUCCESS ({end_time - start_time:.2f}s)\")\n",
        "        print(f\"   Preview: {response.text[:80]}...\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        inference_results[model_name] = {\n",
        "            'status': '‚ùå FAILED',\n",
        "            'response_time': None,\n",
        "            'response_preview': None,\n",
        "            'error': str(e)\n",
        "        }\n",
        "        print(f\"‚ùå Inference: FAILED - {str(e)[:100]}...\")\n",
        "        continue  # Skip fine-tuning test if inference fails\n",
        "    \n",
        "    # Test 2: Fine-tuning (only if inference works)\n",
        "    try:\n",
        "        print(f\"üîß Testing fine-tuning...\")\n",
        "        tuning_job = sft.train(\n",
        "            source_model=model_name,\n",
        "            train_dataset=TRAINING_DATASET,\n",
        "            tuned_model_display_name=f'test-tuning-{model_name.replace(\"-\", \"_\")}',\n",
        "        )\n",
        "        \n",
        "        fine_tuning_results[model_name] = {\n",
        "            'status': '‚úÖ SUPPORTED',\n",
        "            'endpoint': tuning_job.tuned_model_endpoint_name,\n",
        "            'error': None\n",
        "        }\n",
        "        print(f\"‚úÖ Fine-tuning: SUPPORTED\")\n",
        "        print(f\"   Endpoint: {tuning_job.tuned_model_endpoint_name}\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        fine_tuning_results[model_name] = {\n",
        "            'status': '‚ùå NOT SUPPORTED',\n",
        "            'endpoint': None,\n",
        "            'error': str(e)\n",
        "        }\n",
        "        print(f\"‚ùå Fine-tuning: NOT SUPPORTED - {str(e)[:100]}...\")\n",
        "\n",
        "# Summary Report\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"üìä TESTING SUMMARY REPORT\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "print(\"\\nüîπ INFERENCE RESULTS:\")\n",
        "print(\"-\" * 30)\n",
        "for model, result in inference_results.items():\n",
        "    status_emoji = \"‚úÖ\" if result['status'].startswith(\"‚úÖ\") else \"‚ùå\"\n",
        "    print(f\"{status_emoji} {model}: {result['status']}\")\n",
        "\n",
        "print(\"\\nüîπ FINE-TUNING RESULTS:\")\n",
        "print(\"-\" * 30)\n",
        "for model, result in fine_tuning_results.items():\n",
        "    status_emoji = \"‚úÖ\" if result['status'].startswith(\"‚úÖ\") else \"‚ùå\"\n",
        "    print(f\"{status_emoji} {model}: {result['status']}\")\n",
        "\n",
        "# Recommendations\n",
        "print(\"\\nüéØ RECOMMENDATIONS:\")\n",
        "print(\"-\" * 20)\n",
        "working_models = [m for m, r in inference_results.items() if r['status'].startswith(\"‚úÖ\")]\n",
        "fine_tuning_models = [m for m, r in fine_tuning_results.items() if r['status'].startswith(\"‚úÖ\")]\n",
        "\n",
        "if working_models:\n",
        "    print(f\"‚úÖ Use these models for inference: {', '.join(working_models[:3])}\")\n",
        "else:\n",
        "    print(\"‚ùå No models working for inference\")\n",
        "\n",
        "if fine_tuning_models:\n",
        "    print(f\"üîß Use these models for fine-tuning: {', '.join(fine_tuning_models)}\")\n",
        "else:\n",
        "    print(\"‚ùå No models support fine-tuning in this region/project\")\n",
        "    print(\"üí° Try using AI Studio API instead of Vertex AI for fine-tuning\")\n",
        "\n",
        "print(f\"\\nüìà Total tested: {len(MODELS_TO_TEST)} models\")\n",
        "print(f\"‚úÖ Inference working: {len(working_models)} models\")\n",
        "print(f\"üîß Fine-tuning supported: {len(fine_tuning_models)} models\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9db6508f",
      "metadata": {},
      "outputs": [],
      "source": [
        "# üß™ Test the tuned model (run this after fine-tuning completes)\n",
        "\n",
        "# Check if we have the tuned model endpoint\n",
        "if 'TUNED_MODEL_ENDPOINT' in globals() and TUNED_MODEL_ENDPOINT:\n",
        "    print(f\"üß™ Testing tuned model: {TUNED_MODEL_ENDPOINT}\")\n",
        "    \n",
        "    try:\n",
        "        tuned_model = GenerativeModel(TUNED_MODEL_ENDPOINT)\n",
        "        \n",
        "        # Test questions\n",
        "        test_questions = [\n",
        "            'What is a large language model?',\n",
        "            'What is machine learning?',\n",
        "            'Explain fine-tuning in AI.'\n",
        "        ]\n",
        "        \n",
        "        for i, question in enumerate(test_questions, 1):\n",
        "            print(f\"\\n‚ùì Question {i}: {question}\")\n",
        "            response = tuned_model.generate_content(question)\n",
        "            print(f\"üí¨ Answer: {response.text}\")\n",
        "            print(\"-\" * 50)\n",
        "        \n",
        "        print(\"\\n‚úÖ Tuned model is working perfectly!\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error testing tuned model: {e}\")\n",
        "        print(f\"üí° Make sure fine-tuning completed successfully\")\n",
        "        \n",
        "elif 'TUNED_MODEL_NAME' in globals() and TUNED_MODEL_NAME:\n",
        "    print(f\"üß™ Testing tuned model: {TUNED_MODEL_NAME}\")\n",
        "    \n",
        "    try:\n",
        "        tuned_model = GenerativeModel(TUNED_MODEL_NAME)\n",
        "        response = tuned_model.generate_content('What is a large language model?')\n",
        "        print(f\"üí¨ Response: {response.text}\")\n",
        "        print(\"\\n‚úÖ Tuned model is working!\")\n",
        "        \n",
        "    except Exception as e:\n",
        "        print(f\"‚ùå Error: {e}\")\n",
        "        \n",
        "else:\n",
        "    print(\"‚ö†Ô∏è No tuned model endpoint found\")\n",
        "    print(\"üí° Steps:\")\n",
        "    print(\"   1. Make sure fine-tuning cell completed successfully\")\n",
        "    print(\"   2. Check Cloud Console for the tuned model endpoint\")\n",
        "    print(\"   3. Set: TUNED_MODEL_ENDPOINT = 'your-endpoint-here'\")\n",
        "    print(\"   4. Re-run this cell\")\n",
        "    \n",
        "    # You can manually set the endpoint here:\n",
        "    # TUNED_MODEL_ENDPOINT = \"projects/.../locations/.../endpoints/...\"\n",
        "    # tuned_model = GenerativeModel(TUNED_MODEL_ENDPOINT)\n",
        "    # response = tuned_model.generate_content('What is a large language model?')\n",
        "    # print(response.text)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "1117914a",
      "metadata": {},
      "source": [
        "Snippet 2. Using Vertex AI and Google AI studio SDKs for unimodal text generation "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c8fee6a9",
      "metadata": {},
      "outputs": [],
      "source": [
        "%pip install --upgrade --quiet google-genai"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5d70fc80",
      "metadata": {},
      "outputs": [],
      "source": [
        "import sys \n",
        "if \"google.colab\" in sys.modules:\n",
        "    from google.colab import auth  \n",
        "    auth.authenticate_user() \n",
        "from IPython.display import HTML, Markdown, display\n",
        "from google import genai\n",
        "from google.genai.types import (\n",
        "    FunctionDeclaration,\n",
        "    GenerateContentConfig,\n",
        "    GoogleSearch,\n",
        "    HarmBlockThreshold,\n",
        "    HarmCategory,\n",
        "    MediaResolution,\n",
        "    Part,\n",
        "    Retrieval,\n",
        "    SafetySetting,\n",
        "    Tool,\n",
        "    ToolCodeExecution,\n",
        "    VertexAISearch,\n",
        ")\n",
        "import os\n",
        "\n",
        "load_dotenv()\n",
        "\n",
        "\n",
        "PROJECT_ID = os.getenv('PROJECT_ID')  # @param {type: ‚Äústring‚Äù, placeholder: ‚Äú[your-project-id]‚Äù, isTemplate: true}\n",
        "if not PROJECT_ID or PROJECT_ID == PROJECT_ID:\n",
        "    PROJECT_ID = str(os.environ.get(PROJECT_ID)) \n",
        "LOCATION = os.environ.get(REGION)  \n",
        "client = genai.Client(vertexai=True, project=PROJECT_ID, location=LOCATION) \n",
        "MODEL_ID = \"gemini-2.0-flash-001\" # @param {type: ‚Äústring‚Äù} \n",
        "response = client.models.generate_content(\n",
        "    model=MODEL_ID, contents=\"What‚Äôs the largest planet in our solar system?\"\n",
        ")\n",
        "display(Markdown(response.text))"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Codecademy ML",
      "language": "python",
      "name": "codeacademy"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.11.7"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
