# ğŸ”„ Gemini 2.5 Fine-Tuning Process Flowchart

## Overview
This flowchart illustrates the complete fine-tuning process for Gemini 2.5 models based on the whitepaper exercise and notebook implementation.

---

## ğŸ“Š Complete Fine-Tuning Workflow

```mermaid
graph TD
    A[ğŸš€ Start Fine-Tuning Process] --> B[ğŸ“‹ Setup & Configuration]
    B --> C[ğŸ”§ Dataset Creation]
    C --> D[ğŸ“¤ Upload to GCS]
    D --> E[ğŸ¯ Start Training Job]
    E --> F[â³ Monitor Progress]
    F --> G{Job Complete?}
    G -->|No| F
    G -->|Yes| H[âœ… Get Tuned Model]
    H --> I[ğŸ§ª Test & Validate]
    I --> J[ğŸš€ Deploy Model]

    %% Styling
    classDef startEnd fill:#e1f5fe,stroke:#01579b,stroke-width:3px
    classDef process fill:#f3e5f5,stroke:#4a148c,stroke-width:2px
    classDef decision fill:#fff3e0,stroke:#e65100,stroke-width:2px
    classDef success fill:#e8f5e8,stroke:#2e7d32,stroke-width:2px
    classDef critical fill:#ffebee,stroke:#c62828,stroke-width:2px

    class A,J startEnd
    class B,C,D,E,F,H,I process
    class G decision
    class H,I success
    class C,D critical
```

---

## ğŸ” Detailed Process Breakdown

### Phase 1: Setup & Configuration
```mermaid
graph LR
    A[ğŸ”§ Project Setup] --> B[ğŸŒ Region Selection]
    B --> C[ğŸ”‘ Authentication]
    C --> D[ğŸ“¦ SDK Installation]
    D --> E[âœ… Validation]
    
    A1[PROJECT_ID: periospot-mvp] --> A
    B1[REGION: us-central1] --> B
    C1[gcloud auth login] --> C
    D1[google-cloud-aiplatform] --> D
```

### Phase 2: Dataset Creation & Formatting
```mermaid
graph TD
    A[ğŸ“ Create Training Examples] --> B{Format Check}
    B -->|âŒ Old Format| C[âŒ input_text/output_text]
    B -->|âœ… New Format| D[âœ… contents structure]
    
    C --> E[ğŸ”„ Convert to New Format]
    E --> D
    
    D --> F[ğŸ“„ Save as JSONL]
    F --> G[ğŸ” Validate Structure]
    
    subgraph "Correct Format Example"
        H["{<br/>  'contents': [<br/>    {'role': 'user', 'parts': [{'text': 'Q'}]},<br/>    {'role': 'model', 'parts': [{'text': 'A'}]}<br/>  ]<br/>}"]
    end
    
    G --> H
```

### Phase 3: Google Cloud Storage Upload
```mermaid
graph TD
    A[ğŸ“ Local JSONL File] --> B[â˜ï¸ Create GCS Bucket]
    B --> C[ğŸ“¤ Upload File]
    C --> D[ğŸ”— Generate GCS Path]
    D --> E[âœ… Verify Upload]
    
    B1[bucket: periospot-mvp-gemini-tuning] --> B
    C1[gs://bucket/training_data.jsonl] --> C
    D1[gs://periospot-mvp-gemini-tuning/...] --> D
```

### Phase 4: Fine-Tuning Execution
```mermaid
graph TD
    A[ğŸ¯ Configure Training] --> B[ğŸš€ Start Job]
    B --> C[ğŸ“Š Job Monitoring]
    C --> D{Status Check}
    D -->|PENDING| E[â±ï¸ Wait 30s]
    D -->|RUNNING| E
    D -->|SUCCEEDED| F[âœ… Complete]
    D -->|FAILED| G[âŒ Error]
    
    E --> C
    
    subgraph "Training Configuration"
        H["source_model: gemini-2.5-pro<br/>train_dataset: gs://...<br/>tuned_model_name: custom-v1"]
    end
    
    A --> H
```

### Phase 5: Model Testing & Validation
```mermaid
graph TD
    A[ğŸ¯ Get Tuned Endpoint] --> B[ğŸ§ª Test Questions]
    B --> C[ğŸ“Š Compare Responses]
    C --> D[ğŸ“ˆ Performance Analysis]
    D --> E{Performance OK?}
    E -->|Yes| F[âœ… Deploy]
    E -->|No| G[ğŸ”„ Retrain/Adjust]
    
    subgraph "Test Questions"
        H["What is machine learning?<br/>Explain neural networks<br/>What is fine-tuning?"]
    end
    
    B --> H
```

---

## ğŸ¯ Key Training Data (From Whitepaper Exercise)

### Training Examples Used:
1. **Machine Learning Fundamentals**
   - Question: "What is machine learning?"
   - Answer: Comprehensive explanation of ML as AI subset

2. **Neural Networks Education**
   - Question: "Explain neural networks in simple terms"
   - Answer: Biological inspiration and computing systems

3. **Fine-Tuning Concepts**
   - Question: "What is fine-tuning in AI?"
   - Answer: Process of adapting pre-trained models

### Data Format Transformation:
```mermaid
graph LR
    A[âŒ Old Format] --> B[âœ… New Format]
    
    subgraph "Old (Doesn't Work)"
        C["{<br/>  'input_text': 'Question',<br/>  'output_text': 'Answer'<br/>}"]
    end
    
    subgraph "New (Required)"
        D["{<br/>  'contents': [<br/>    {'role': 'user', 'parts': [...]},<br/>    {'role': 'model', 'parts': [...]}<br/>  ]<br/>}"]
    end
    
    A --> C
    B --> D
```

---

## ğŸ”§ Technical Requirements

### Environment Setup:
- **Project**: periospot-mvp
- **Region**: us-central1 (required for fine-tuning)
- **Model**: gemini-2.5-pro (or gemini-2.5-flash)
- **Storage**: Google Cloud Storage bucket
- **Authentication**: gcloud auth application-default login

### Critical Success Factors:
1. âœ… **Correct Dataset Format** - Use `contents` structure
2. âœ… **GCS Upload** - Local files not accepted
3. âœ… **Supported Model** - Only Gemini 2.5 models
4. âœ… **Proper Region** - us-central1 required
5. âœ… **Valid Authentication** - Google Cloud credentials

---

## ğŸ“Š Expected Outcomes

### Before Fine-Tuning:
- Generic AI responses to ML questions
- Inconsistent terminology and depth
- General-purpose knowledge

### After Fine-Tuning:
- Specialized AI/ML explanations
- Consistent educational tone
- Domain-specific expertise
- Improved accuracy on ML topics

---

## ğŸš¨ Common Issues & Solutions

```mermaid
graph TD
    A[âŒ Error] --> B{Error Type}
    
    B -->|Missing contents field| C[ğŸ”§ Fix Dataset Format]
    B -->|Local file error| D[ğŸ“¤ Upload to GCS]
    B -->|Model not supported| E[ğŸ¯ Use Gemini 2.5]
    B -->|Region error| F[ğŸŒ Switch to us-central1]
    B -->|Auth error| G[ğŸ”‘ Re-authenticate]
    
    C --> H[âœ… Retry Training]
    D --> H
    E --> H
    F --> H
    G --> H
```

---

## ğŸ“ Learning Objectives (From Whitepaper)

This exercise demonstrates:
1. **Dataset Format Evolution** - Understanding new Gemini 2.5 requirements
2. **Cloud Infrastructure** - GCS integration for training data
3. **Model Specialization** - Creating domain-specific AI models
4. **Production Workflow** - End-to-end fine-tuning pipeline
5. **Error Handling** - Debugging and troubleshooting techniques

---

## ğŸ“ˆ Performance Metrics

### Training Metrics:
- **Dataset Size**: 3 examples (demonstration)
- **Training Time**: 15-30 minutes
- **Model Size**: Same as base model
- **Cost**: Based on Vertex AI pricing

### Quality Metrics:
- **Accuracy**: Improved on AI/ML topics
- **Consistency**: Standardized explanations
- **Relevance**: Domain-focused responses
- **Educational Value**: Clear, structured answers

---

**Status**: âœ… **Working Solution** - Complete fine-tuning pipeline operational

**Last Updated**: January 2025  
**Based On**: Whitepaper exercise + Notebook implementation
